# LangChain & LangSmith Integration Guide

> **Complete guide to understanding and using LangChain/LangGraph with LangSmith observability in Second Brain Database**

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [LangChain Components](#langchain-components)
4. [LangSmith Observability](#langsmith-observability)
5. [Configuration](#configuration)
6. [Testing & Debugging](#testing--debugging)
7. [Production Deployment](#production-deployment)

---

## Overview

### What You Have Built

Your Second Brain Database uses **LangChain/LangGraph** to create AI agents that can:

1. **Access 53 MCP Tools** - Complete coverage of all backend functionality
2. **Manage Context** - Redis-backed conversation memory
3. **Execute Workflows** - Multi-step reasoning with ReAct pattern
4. **Stream Responses** - Real-time token-level streaming
5. **Handle Permissions** - Role-based tool access control

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Framework** | LangChain 0.3.20 | Core agent framework |
| **Agent Type** | LangGraph ReAct | Multi-step reasoning |
| **LLM** | Ollama (llama3.2:1b) | Local tool-calling model |
| **Memory** | Redis + RedisChatMessageHistory | Conversation persistence |
| **Tools** | FastMCP 2.x â†’ LangChain | MCP to LangChain bridge |
| **Observability** | LangSmith | Tracing & monitoring |

---

## Architecture

### High-Level Flow

```
User Request
    â†“
FastAPI Endpoint
    â†“
LangChainOrchestrator
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangGraph ReAct Agent              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. Reason about task         â”‚   â”‚
â”‚  â”‚ 2. Select appropriate tool   â”‚   â”‚
â”‚  â”‚ 3. Execute MCP tool          â”‚   â”‚
â”‚  â”‚ 4. Observe result            â”‚   â”‚
â”‚  â”‚ 5. Continue or respond       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Redis Memory (History)
    â†“
LangSmith (Tracing)
    â†“
Response to User
```

### Component Breakdown

#### 1. **LangChainOrchestrator** (`orchestrator.py`)

The main orchestration class that:

```python
class LangChainOrchestrator:
    """Orchestrates LangChain agents with MCP tools."""
    
    def __init__(self, settings, redis_manager):
        # Initialize LLM (Ollama)
        self.llm = ChatOllama(
            model="llama3.2:1b",
            base_url="http://127.0.0.1:11434",
            temperature=0.7,
            num_predict=2048,
        )
        
        # Initialize Redis-backed memory
        self.memory = MCPIntegratedMemory(
            redis_manager=redis_manager,
            conversation_history_limit=50,
            ttl=3600,
        )
        
        # Agent cache
        self.agents = {}
```

**Key Methods:**

- `_create_tools_for_user()` - Creates tools based on user permissions
- `_create_agent()` - Creates a LangGraph ReAct agent
- `chat()` - Processes user messages with agent

#### 2. **Tool Creation** (`tools/`)

Tools are created from MCP functions:

```python
# tools/base.py
def create_langchain_tool_from_mcp(
    mcp_function: Callable,
    user_context: MCPUserContext
) -> StructuredTool:
    """Convert FastMCP FunctionTool to LangChain StructuredTool."""
    
    # Extract function from FastMCP FunctionTool
    if hasattr(mcp_function, 'fn'):
        actual_function = mcp_function.fn
    else:
        actual_function = mcp_function
    
    # Create wrapper with user context
    wrapper = MCPToolWrapper(actual_function, user_context)
    
    # Return LangChain tool
    return StructuredTool.from_function(
        func=wrapper.execute,
        name=actual_function.__name__,
        description=actual_function.__doc__ or f"Execute {actual_function.__name__}"
    )
```

**Tool Categories:**

- **Family Tools** (1 visible) - Family management
- **Shop Tools** (2 visible) - Digital shop operations
- **Auth Tools** (3 visible) - Authentication & security
- **Workspace Tools** (23 visible) - Team collaboration
- **Admin Tools** (1 visible for admins) - System administration

Total: **33 tools for admin users**

#### 3. **Memory System** (`memory/redis_memory.py`)

Redis-backed conversation history:

```python
class MCPIntegratedMemory:
    """Redis-backed conversation memory for LangChain agents."""
    
    def __init__(self, redis_manager, conversation_history_limit=50, ttl=3600):
        self.redis_manager = redis_manager
        self.conversation_history_limit = conversation_history_limit
        self.ttl = ttl
        self.sessions = {}
    
    def get_chat_history(self, session_id: str) -> RedisChatMessageHistory:
        """Get or create chat history for session."""
        if session_id not in self.sessions:
            redis_client = self.redis_manager.get_sync_redis()
            self.sessions[session_id] = RedisChatMessageHistory(
                session_id=session_id,
                redis_client=redis_client,
                ttl=self.ttl
            )
        return self.sessions[session_id]
    
    def trim_history(self, session_id: str):
        """Trim conversation history to limit."""
        history = self.get_chat_history(session_id)
        if len(history.messages) > self.conversation_history_limit:
            # Keep only recent messages
            history.messages = history.messages[-self.conversation_history_limit:]
```

**Features:**

- âœ… Persistent across restarts (Redis)
- âœ… Automatic expiration (TTL)
- âœ… Trimming to prevent memory bloat
- âœ… Sync Redis client (LangChain compatible)

---

## LangChain Components

### 1. Agent Type: ReAct (Reasoning + Acting)

Your system uses **LangGraph's ReAct agent**:

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    llm,              # ChatOllama
    tools,            # List of LangChain tools
    prompt=system_message,  # System instruction
)
```

**How ReAct Works:**

1. **Thought** - Agent reasons about what to do
2. **Action** - Selects a tool and parameters
3. **Observation** - Sees the tool result
4. **Repeat** - Continue until answer is complete
5. **Final Answer** - Responds to user

**Example Execution:**

```
User: "What families am I in?"

Thought: I need to check the user's family memberships
Action: get_family_info(include_members=True)
Observation: {"families": [{"id": "123", "name": "Smith Family", ...}]}
Thought: I now have the information needed
Final Answer: "You are in 1 family: Smith Family"
```

### 2. LLM: Ollama (llama3.2:1b)

**Why llama3.2?**

- âœ… Supports **tool calling** (function calling)
- âœ… Small model (1B parameters) = fast responses
- âœ… Runs locally (no API costs)
- âœ… Good for reasoning tasks

**Configuration:**

```python
llm = ChatOllama(
    model="llama3.2:1b",                # Model name
    base_url="http://127.0.0.1:11434",  # Ollama server
    temperature=0.7,                     # Creativity (0-2)
    num_predict=2048,                    # Max tokens
)
```

**Alternative Models:**

You can switch to other models by changing config:

```bash
# In .env or .sbd file
LANGCHAIN_DEFAULT_MODEL=deepseek-r1:1.5b  # DeepSeek reasoning
LANGCHAIN_DEFAULT_MODEL=llama3.2:3b       # Larger Llama
LANGCHAIN_DEFAULT_MODEL=gemma2:2b         # Google Gemma
```

### 3. Tools: MCP â†’ LangChain Bridge

**Tool Creation Pipeline:**

```
FastMCP @authenticated_tool
    â†“
FunctionTool with .fn attribute
    â†“
MCPToolWrapper (adds user context)
    â†“
LangChain StructuredTool
    â†“
Available to Agent
```

**Example:**

```python
# MCP Tool
@authenticated_tool(
    name="get_family_info",
    description="Get family information",
    permissions=["family:read"],
)
async def get_family_info(family_id: str) -> Dict[str, Any]:
    """Get detailed information about a family."""
    # ... implementation

# Converted to LangChain Tool
tool = create_langchain_tool_from_mcp(
    mcp_function=get_family_info,
    user_context=user_context
)

# Agent can now call:
result = await agent.ainvoke({
    "messages": [HumanMessage("Tell me about family 123")]
})
```

### 4. Memory: Redis-Backed History

**Storage Structure:**

```
Redis Key: chat_history:{session_id}
Value: JSON array of messages

[
  {"type": "human", "content": "Hello"},
  {"type": "ai", "content": "Hi! How can I help?"},
  {"type": "human", "content": "What families am I in?"},
  {"type": "ai", "content": "You are in Smith Family"},
]
```

**Features:**

- Automatic serialization/deserialization
- TTL expiration (default: 1 hour)
- Message trimming (max 50 messages)
- Session isolation

---

## LangSmith Observability

### What is LangSmith?

**LangSmith** is LangChain's observability platform that provides:

- ðŸ” **Tracing** - See every step of agent execution
- ðŸ“Š **Analytics** - Performance metrics and costs
- ðŸ› **Debugging** - Inspect failures and errors
- ðŸ“ˆ **Monitoring** - Production usage patterns
- ðŸ§ª **Testing** - Evaluate agent performance

### Current Configuration

Your system is **configured but not enabled**:

```python
# config.py
LANGSMITH_API_KEY: Optional[str] = None  # âŒ Not set
LANGSMITH_PROJECT: str = "SecondBrainDatabase"  # âœ… Project name
LANGSMITH_ENDPOINT: str = "https://api.smith.langchain.com"  # âœ… Endpoint
LANGSMITH_TRACING: bool = False  # âŒ Disabled

# Also:
LANGCHAIN_TRACING_V2: str = "false"  # âŒ Disabled
LANGCHAIN_PROJECT: str = "SecondBrainDatabase"  # âœ… Project name
```

### How to Enable LangSmith

#### Step 1: Get API Key

1. Go to [smith.langchain.com](https://smith.langchain.com)
2. Sign up or log in
3. Go to Settings â†’ API Keys
4. Create a new API key
5. Copy the key (starts with `lsv2_pt_...`)

#### Step 2: Configure Environment

Add to your `.env` or `.sbd` file:

```bash
# LangSmith Configuration
LANGSMITH_API_KEY=lsv2_pt_your_actual_api_key_here
LANGSMITH_TRACING=true
LANGCHAIN_TRACING_V2=true
```

Or set environment variables:

```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=lsv2_pt_your_actual_api_key_here
export LANGCHAIN_PROJECT=SecondBrainDatabase
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

#### Step 3: Verify Setup

Run your test:

```bash
python3 final_coverage_test.py
```

Then check [smith.langchain.com/SecondBrainDatabase](https://smith.langchain.com) to see traces!

### What You'll See in LangSmith

When tracing is enabled, every agent execution shows:

```
Session: user_123_session_456
â”œâ”€ Agent Start
â”‚  â””â”€ Input: "What families am I in?"
â”œâ”€ LLM Call (llama3.2:1b)
â”‚  â”œâ”€ Prompt: [System + History + User message]
â”‚  â”œâ”€ Response: {thought: "Need to check families", action: "get_family_info"}
â”‚  â””â”€ Duration: 234ms, Tokens: 145
â”œâ”€ Tool: get_family_info
â”‚  â”œâ”€ Input: {family_id: "123"}
â”‚  â”œâ”€ Output: {families: [...]}
â”‚  â””â”€ Duration: 42ms
â”œâ”€ LLM Call (llama3.2:1b)
â”‚  â”œâ”€ Prompt: [Previous + Tool result]
â”‚  â”œâ”€ Response: "You are in 1 family: Smith Family"
â”‚  â””â”€ Duration: 189ms, Tokens: 87
â””â”€ Agent End
   â””â”€ Output: "You are in 1 family: Smith Family"
   
Total Duration: 465ms
Total Tokens: 232
```

### Advanced LangSmith Features

Once enabled, you can:

#### 1. **Monitor Performance**

```python
# View in LangSmith UI:
# - Average response time
# - Token usage by model
# - Error rates
# - Most used tools
```

#### 2. **Debug Failures**

```python
# See exact failure points:
# - Which tool failed?
# - What was the error?
# - What inputs caused it?
```

#### 3. **Compare Runs**

```python
# Compare different:
# - Models (llama3.2 vs deepseek)
# - Temperatures (0.7 vs 0.0)
# - System prompts
# - Tool sets
```

#### 4. **Create Datasets**

```python
# Test against known scenarios:
# 1. Create dataset of test cases
# 2. Run agent against all cases
# 3. View success/failure rates
```

### LangSmith Integration Code

The integration happens **automatically** when environment variables are set:

```python
# No code changes needed!
# LangChain detects these environment variables:
# - LANGCHAIN_TRACING_V2=true
# - LANGCHAIN_API_KEY=lsv2_pt_...
# - LANGCHAIN_PROJECT=SecondBrainDatabase

# All agent executions are automatically traced
```

---

## Configuration

### Environment Variables

```bash
# LangChain Core
LANGCHAIN_ENABLED=true
LANGCHAIN_MODEL_PROVIDER=ollama
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b
LANGCHAIN_TEMPERATURE=0.7
LANGCHAIN_MAX_TOKENS=2048

# Memory Settings
LANGCHAIN_MEMORY_TTL=3600
LANGCHAIN_CONVERSATION_HISTORY_LIMIT=50

# Rate Limiting
LANGCHAIN_RATE_LIMIT_REQUESTS=100
LANGCHAIN_MAX_CONCURRENT_SESSIONS=1000

# LangSmith Tracing
LANGCHAIN_TRACING_V2=false  # Set to 'true' to enable
LANGCHAIN_PROJECT=SecondBrainDatabase
# LANGCHAIN_API_KEY=  # Your LangSmith API key
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# Ollama Settings
OLLAMA_HOST=http://127.0.0.1:11434

# Redis (for memory)
REDIS_HOST=127.0.0.1
REDIS_PORT=6379
REDIS_DB=0
```

### Tool Categories Configuration

```bash
# Enable/disable tool categories
MCP_FAMILY_TOOLS_ENABLED=true
MCP_SHOP_TOOLS_ENABLED=true
MCP_AUTH_TOOLS_ENABLED=true
MCP_WORKSPACE_TOOLS_ENABLED=true
MCP_ADMIN_TOOLS_ENABLED=false  # Admin only
```

### Model Selection

#### Ollama Models (Local)

```bash
# Fast, small models
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b      # Best for tool calling
LANGCHAIN_DEFAULT_MODEL=gemma2:2b        # Google's Gemma
LANGCHAIN_DEFAULT_MODEL=deepseek-r1:1.5b # Reasoning model

# Larger, more capable
LANGCHAIN_DEFAULT_MODEL=llama3.2:3b
LANGCHAIN_DEFAULT_MODEL=qwen2.5:7b
```

#### Cloud Models (Requires API keys)

```bash
# OpenAI
LANGCHAIN_MODEL_PROVIDER=openai
LANGCHAIN_DEFAULT_MODEL=gpt-4-turbo
OPENAI_API_KEY=sk-...

# Anthropic
LANGCHAIN_MODEL_PROVIDER=anthropic
LANGCHAIN_DEFAULT_MODEL=claude-3-sonnet-20240229
ANTHROPIC_API_KEY=sk-ant-...
```

---

## Testing & Debugging

### Running the Test Suite

Your existing test file:

```bash
python3 final_coverage_test.py
```

**What it tests:**

1. Tool loading (33 tools for admin)
2. Permission-based filtering
3. Tool categories (Family, Shop, Auth, Workspace, Admin)
4. Memory system
5. Agent creation

### Manual Testing

Test the orchestrator directly:

```python
# test_langchain_agent.py
import asyncio
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager
from src.second_brain_database.integrations.mcp.context import MCPUserContext

async def test_agent():
    settings = Settings()
    redis_manager = RedisManager()
    
    orchestrator = LangChainOrchestrator(settings, redis_manager)
    
    # Create test user context
    user_context = MCPUserContext(
        user_id="test_user",
        username="test",
        role="admin",
        permissions=["admin", "family:read", "shop:read"]
    )
    
    # Chat with agent
    response = await orchestrator.chat(
        session_id="test_session",
        user_id="test_user",
        message="What tools can you use?",
        user_context=user_context,
        agent_type="general"
    )
    
    print(f"Response: {response['response']}")

asyncio.run(test_agent())
```

### Debugging Tips

#### 1. **Enable Debug Logging**

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### 2. **Check Tool Availability**

```python
orchestrator = LangChainOrchestrator(settings, redis_manager)
tools = orchestrator._create_tools_for_user(user_context)
print(f"Available tools: {[t.name for t in tools]}")
```

#### 3. **Inspect Memory**

```python
history = orchestrator.memory.get_chat_history("session_id")
print(f"Message count: {len(history.messages)}")
for msg in history.messages:
    print(f"{msg.type}: {msg.content}")
```

#### 4. **Test Individual Tools**

```python
from src.second_brain_database.integrations.langchain.tools.family_tools import create_family_tools

tools = create_family_tools(user_context)
for tool in tools:
    print(f"Tool: {tool.name}")
    print(f"Description: {tool.description}")
```

### Common Issues

#### Issue: "Tool already exists"

**Cause:** Duplicate tool registrations

**Solution:** We fixed this by disabling old registration files:
- `resources_registration.py` â†’ Commented out
- `prompts_registration.py` â†’ Commented out
- Removed duplicate `designate_backup_admin` from `family_tools.py`

#### Issue: "Model doesn't support tool calling"

**Cause:** Using a model without function calling support (e.g., `gemma3:1b`)

**Solution:** Use `llama3.2:1b` or other tool-calling models:
```bash
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b
```

#### Issue: "Redis connection failed"

**Cause:** Redis not running

**Solution:** Start Redis:
```bash
redis-server --daemonize yes
# or
brew services start redis
```

---

## Production Deployment

### Prerequisites

1. âœ… **Redis** - Running and accessible
2. âœ… **Ollama** - Running with llama3.2:1b model
3. âœ… **MongoDB** - For user context and data
4. âœ… **LangSmith** (Optional) - For production monitoring

### Production Checklist

#### 1. **Enable LangSmith Tracing**

```bash
# .env or .sbd
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_your_production_key
```

#### 2. **Adjust Rate Limits**

```bash
# Prevent abuse
LANGCHAIN_RATE_LIMIT_REQUESTS=100
LANGCHAIN_MAX_CONCURRENT_SESSIONS=1000
```

#### 3. **Configure Memory TTL**

```bash
# Balance between context and memory usage
LANGCHAIN_MEMORY_TTL=3600  # 1 hour
LANGCHAIN_CONVERSATION_HISTORY_LIMIT=50  # Max 50 messages
```

#### 4. **Monitor Performance**

Use LangSmith to track:
- Response times
- Token usage
- Error rates
- Tool usage patterns

#### 5. **Set Up Alerts**

In LangSmith, configure alerts for:
- Response time > 5 seconds
- Error rate > 5%
- Token usage spikes

### Performance Optimization

#### 1. **Use Smaller Models**

```bash
# Faster responses
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b  # 1B params = fast
```

#### 2. **Cache Agent Instances**

Already implemented in orchestrator:

```python
# Agents are cached per user+type
agent_key = f"{user_id}:{agent_type}"
if agent_key not in self.agents:
    self.agents[agent_key] = self._create_agent(...)
```

#### 3. **Trim Conversation History**

Already implemented:

```python
def trim_history(self, session_id: str):
    """Keep only recent messages."""
    if len(history.messages) > self.conversation_history_limit:
        history.messages = history.messages[-self.conversation_history_limit:]
```

#### 4. **Use Connection Pooling**

Redis manager already uses connection pooling.

### Monitoring Metrics

Track these metrics in production:

| Metric | Target | Action if Exceeded |
|--------|--------|-------------------|
| Avg Response Time | < 2s | Scale Ollama, use smaller model |
| Error Rate | < 1% | Check tool errors, model issues |
| Token Usage | < 1000/req | Reduce context, trim history |
| Redis Memory | < 1GB | Reduce TTL, trim more aggressively |
| Concurrent Sessions | < 1000 | Scale horizontally |

---

## Summary

### What You Have

âœ… **Full LangChain Integration**
- 53 MCP tools wrapped as LangChain tools
- ReAct agent with multi-step reasoning
- Redis-backed conversation memory
- Permission-based tool access

âœ… **Production-Ready Architecture**
- Agent caching
- Memory trimming
- Rate limiting
- Error handling

âœ… **LangSmith Ready**
- Configuration in place
- Just need to add API key
- Automatic tracing once enabled

### Quick Start Commands

```bash
# 1. Enable LangSmith (optional but recommended)
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=lsv2_pt_your_key_here

# 2. Run the test
python3 final_coverage_test.py

# 3. Check LangSmith
# Visit: https://smith.langchain.com/SecondBrainDatabase

# 4. Test manually
python3 -c "
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager

settings = Settings()
redis_manager = RedisManager()
orchestrator = LangChainOrchestrator(settings, redis_manager)
print(f'Orchestrator ready with model: {settings.LANGCHAIN_DEFAULT_MODEL}')
"
```

### Next Steps

1. **Enable LangSmith** - Get full observability
2. **Test Different Models** - Compare llama3.2 vs deepseek
3. **Create Test Datasets** - Build evaluation suite
4. **Monitor Production** - Track metrics and errors
5. **Optimize Performance** - Based on real usage patterns

---

**Documentation Generated:** 2 November 2025  
**System:** Second Brain Database v1.0.0  
**LangChain Version:** 0.3.20  
**FastMCP Version:** 2.13.0.2
