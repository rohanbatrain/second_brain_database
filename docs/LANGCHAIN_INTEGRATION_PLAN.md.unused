# LangChain/LangGraph AI Agent Integration Plan
## Production Implementation Roadmap

**Status**: Ready for Implementation  
**Timeline**: 4-6 weeks  
**Priority**: High  
**Complexity**: High  

---

## Executive Summary

This plan details the integration of **LangChain/LangGraph AI agent orchestration** into the existing Second Brain Database architecture. The implementation leverages:

- **Existing MCP tools** (138+ tools across 5 categories)
- **Current managers** (FamilyManager, SecurityManager, RedisManager, etc.)
- **Established authentication** (JWT, 2FA, permanent tokens)
- **Production infrastructure** (MongoDB, Redis, FastAPI, LiveKit)

The system will enable natural language interactions with all existing functionality while maintaining security, performance, and scalability.

---

## System Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                     EXISTING FASTAPI APPLICATION                │
├─────────────────────────────────────────────────────────────────┤
│  NEW: /api/v1/ai/chat         - Text agent interactions         │
│  NEW: /api/v1/ai/voice        - Voice agent interactions        │
│  NEW: /api/v1/ai/sessions     - Session management              │
│  NEW: /api/v1/ai/workflows    - LangGraph workflow execution    │
│  NEW: WS  /ws/ai/stream       - Real-time streaming responses   │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│             LANGCHAIN AGENT ORCHESTRATION LAYER (NEW)           │
├─────────────────────────────────────────────────────────────────┤
│  LangChainOrchestrator  → Routes requests to specialized agents │
│  AgentExecutor          → Manages tool calling and reasoning    │
│  LangGraph Engine       → Handles complex multi-step workflows  │
│  Memory Manager         → Conversation history & context        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                  SPECIALIZED AI AGENTS (NEW)                    │
├─────────────────────────────────────────────────────────────────┤
│  FamilyAgent      → Family management & SBD tokens              │
│  PersonalAgent    → User profile, auth, 2FA                     │
│  WorkspaceAgent   → Team collaboration & projects               │
│  CommerceAgent    → Shopping, assets, transactions              │
│  SecurityAgent    → Admin operations & monitoring               │
│  VoiceAgent       → Voice command processing                    │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│             MCP TOOL WRAPPERS (NEW INTEGRATION LAYER)           │
├─────────────────────────────────────────────────────────────────┤
│  LangChain BaseTool wrappers for:                               │
│    - family_tools.py (30+ tools)                                │
│    - auth_tools.py (30+ tools)                                  │
│    - shop_tools.py (35+ tools)                                  │
│    - workspace_tools.py (25+ tools)                             │
│    - admin_tools.py (23+ tools)                                 │
│                                                                  │
│  Preserves: MCPUserContext, security decorators, audit logging  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                 EXISTING MCP INFRASTRUCTURE                     │
├─────────────────────────────────────────────────────────────────┤
│  FastMCP Server 2.13.0.2  → Tool registration & execution       │
│  MCPUserContext           → User authentication & permissions   │
│  Security Decorators      → @authenticated_tool, rate limiting  │
│  Audit Logging            → Complete operation tracking         │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                    EXISTING MANAGERS                            │
├─────────────────────────────────────────────────────────────────┤
│  FamilyManager      → Family CRUD, members, invitations         │
│  SecurityManager    → Rate limiting, IP lockdown, 2FA           │
│  RedisManager       → Caching, sessions, rate limits            │
│  WorkspaceManager   → Team workspaces, roles, permissions       │
│  LoggingManager     → Centralized logging & monitoring          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                   EXISTING DATA LAYER                           │
├─────────────────────────────────────────────────────────────────┤
│  MongoDB            → Primary data store (Motor async driver)   │
│  Redis              → Cache, sessions, rate limits              │
│  LiveKit            → Voice/video infrastructure                │
│  Ollama             → Local LLM inference (llama3.1)            │
└─────────────────────────────────────────────────────────────────┘
```

---

## Phase 1: Foundation & Configuration (Week 1)

### 1.1 Install LangChain Ecosystem

**Objective**: Add LangChain dependencies without breaking existing functionality.

**Tasks**:
```bash
# Add to requirements.txt via uv
uv add langchain==0.3.0
uv add langchain-core==0.3.0  
uv add langchain-community==0.3.0
uv add langgraph==0.2.0
uv add langchain-ollama==0.2.0  # For local LLM integration
uv add langsmith==0.2.0  # For observability
```

**Configuration Updates** (`src/second_brain_database/config.py`):

```python
class Settings(BaseSettings):
    # ... existing settings ...
    
    # ===== LANGCHAIN CONFIGURATION =====
    # Enable/Disable AI Features
    LANGCHAIN_ENABLED: bool = True
    
    # Model Configuration (uses existing Ollama)
    LANGCHAIN_MODEL_PROVIDER: str = "ollama"  # "ollama" or "openai"
    LANGCHAIN_DEFAULT_MODEL: str = "llama3.1"  # Use existing Ollama model
    LANGCHAIN_TEMPERATURE: float = 0.7
    LANGCHAIN_MAX_TOKENS: int = 2000
    
    # Memory & Session Configuration (uses existing Redis)
    LANGCHAIN_MEMORY_TTL: int = 3600  # 1 hour session TTL
    LANGCHAIN_CONVERSATION_HISTORY_LIMIT: int = 50  # Max messages
    
    # Rate Limiting (integrates with existing SecurityManager)
    LANGCHAIN_RATE_LIMIT_REQUESTS: int = 100  # per hour per user
    LANGCHAIN_MAX_CONCURRENT_SESSIONS: int = 10  # per user
    
    # LangSmith Observability (optional)
    LANGCHAIN_TRACING_V2: bool = False
    LANGCHAIN_API_KEY: Optional[str] = None
    LANGCHAIN_PROJECT: str = "SecondBrainDatabase"
```

**Validation**:
- ✅ All dependencies install without conflicts
- ✅ Existing tests pass (no regressions)
- ✅ Configuration loads correctly

---

## Phase 2: MCP Tool Integration Layer (Week 1-2)

### 2.1 Create LangChain Tool Wrappers

**Objective**: Bridge existing MCP tools with LangChain's tool interface.

**File Structure**:
```
src/second_brain_database/integrations/langchain/
├── __init__.py
├── tools/
│   ├── __init__.py
│   ├── base.py                 # MCPToolWrapper base class
│   ├── family_tools.py         # Wrappers for family MCP tools
│   ├── auth_tools.py           # Wrappers for auth MCP tools
│   ├── shop_tools.py           # Wrappers for shop MCP tools
│   ├── workspace_tools.py      # Wrappers for workspace MCP tools
│   └── admin_tools.py          # Wrappers for admin MCP tools
```

**Implementation** (`tools/base.py`):

```python
"""
LangChain tool wrappers for existing MCP tools.

Preserves all existing functionality:
- MCPUserContext authentication
- @authenticated_tool security decorators
- Rate limiting via SecurityManager
- Audit logging
- Error handling patterns
"""

from typing import Any, Dict, Callable, Optional
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
import json
import asyncio

from ....integrations.mcp.context import get_mcp_user_context, MCPUserContext
from ....integrations.mcp.exceptions import (
    MCPAuthenticationError,
    MCPAuthorizationError,
    MCPToolExecutionError
)
from ....managers.logging_manager import get_logger

logger = get_logger(prefix="[LangChain_MCP_Tools]")


class MCPToolWrapper(BaseTool):
    """
    Wrapper that converts existing MCP tools to LangChain tools.
    
    Preserves:
    - Existing authentication via MCPUserContext
    - Security decorators and permission checks
    - Rate limiting through SecurityManager
    - Audit logging and error handling
    - All existing business logic
    """
    
    name: str
    description: str
    mcp_tool_func: Callable
    requires_auth: bool = True
    required_permissions: list[str] = Field(default_factory=list)
    
    class Config:
        arbitrary_types_allowed = True
    
    async def _arun(self, **kwargs) -> str:
        """
        Async execution that integrates with existing MCP infrastructure.
        
        The MCPUserContext is already set by the orchestrator, so tools
        execute with proper user authentication and permissions.
        """
        try:
            # Get user context (set by orchestrator)
            if self.requires_auth:
                user_context = get_mcp_user_context()
                if not user_context:
                    raise MCPAuthenticationError("No user context available")
            
            # Execute the underlying MCP tool
            # The @authenticated_tool decorator handles security checks
            result = await self.mcp_tool_func(**kwargs)
            
            # Log successful execution
            logger.info(
                f"MCP tool {self.name} executed successfully",
                extra={
                    "tool": self.name,
                    "user_id": user_context.user_id if self.requires_auth else None
                }
            )
            
            # Return JSON string for LangChain consumption
            if isinstance(result, dict):
                return json.dumps(result)
            return str(result)
            
        except MCPAuthenticationError as e:
            logger.error(f"Authentication error in MCP tool {self.name}: {e}")
            return json.dumps({"error": "authentication_required", "message": str(e)})
            
        except MCPAuthorizationError as e:
            logger.error(f"Authorization error in MCP tool {self.name}: {e}")
            return json.dumps({"error": "permission_denied", "message": str(e)})
            
        except Exception as e:
            logger.error(
                f"Error executing MCP tool {self.name}: {e}",
                exc_info=True
            )
            return json.dumps({"error": "execution_failed", "message": str(e)})
    
    def _run(self, **kwargs) -> str:
        """Sync wrapper for compatibility."""
        return asyncio.run(self._arun(**kwargs))


def create_mcp_tool_wrapper(
    mcp_tool_func: Callable,
    name: str,
    description: str,
    requires_auth: bool = True,
    required_permissions: list[str] = None
) -> MCPToolWrapper:
    """
    Factory function to create LangChain tool wrappers for MCP tools.
    
    Usage:
        from ...integrations.mcp.tools import family_tools
        
        family_info_tool = create_mcp_tool_wrapper(
            family_tools.get_family_info,
            "get_family_info",
            "Get detailed information about a family including members and roles",
            requires_auth=True,
            required_permissions=["family:read"]
        )
    """
    return MCPToolWrapper(
        name=name,
        description=description,
        mcp_tool_func=mcp_tool_func,
        requires_auth=requires_auth,
        required_permissions=required_permissions or []
    )
```

**Example Family Tools Wrappers** (`tools/family_tools.py`):

```python
"""
LangChain tool wrappers for existing family MCP tools.

Wraps all 30+ family tools from:
  src/second_brain_database/integrations/mcp/tools/family_tools.py
"""

from typing import List
from langchain.tools import BaseTool

from ....integrations.mcp.tools import family_tools as mcp_family
from .base import create_mcp_tool_wrapper

# Family Information Tools
get_family_info = create_mcp_tool_wrapper(
    mcp_family.get_family_info,
    "get_family_info",
    "Get detailed family information including members, roles, and status. "
    "Requires family_id parameter.",
    requires_auth=True,
    required_permissions=["family:read"]
)

get_family_members = create_mcp_tool_wrapper(
    mcp_family.get_family_members,
    "get_family_members",
    "Get all members of a family with their roles and permissions. "
    "Requires family_id parameter.",
    requires_auth=True,
    required_permissions=["family:read"]
)

get_user_families = create_mcp_tool_wrapper(
    mcp_family.get_user_families,
    "get_user_families",
    "Get all families the current user is a member of.",
    requires_auth=True,
    required_permissions=[]
)

# Family Management Tools
create_family = create_mcp_tool_wrapper(
    mcp_family.create_family,
    "create_family",
    "Create a new family with the user as administrator. "
    "Requires name parameter.",
    requires_auth=True,
    required_permissions=["family:create"]
)

send_family_invitation = create_mcp_tool_wrapper(
    mcp_family.send_family_invitation,
    "send_family_invitation",
    "Send invitation to join a family. "
    "Requires family_id and email parameters.",
    requires_auth=True,
    required_permissions=["family:admin"]
)

# SBD Token Tools
create_token_request = create_mcp_tool_wrapper(
    mcp_family.create_token_request,
    "create_token_request",
    "Request SBD tokens from family administrator. "
    "Requires family_id, amount, and reason parameters.",
    requires_auth=True,
    required_permissions=["family:member"]
)

get_family_sbd_account = create_mcp_tool_wrapper(
    mcp_family.get_family_sbd_account,
    "get_family_sbd_account",
    "Get family SBD token account balance and transaction history. "
    "Requires family_id parameter.",
    requires_auth=True,
    required_permissions=["family:read"]
)

# ... Continue for all 30+ family tools ...


def get_all_family_tools() -> List[BaseTool]:
    """
    Get all family-related LangChain tools.
    
    Returns list of LangChain BaseTool instances that wrap
    existing MCP family tools.
    """
    return [
        get_family_info,
        get_family_members,
        get_user_families,
        create_family,
        send_family_invitation,
        create_token_request,
        get_family_sbd_account,
        # ... all other family tools ...
    ]
```

**Validation**:
- ✅ All MCP tools wrapped as LangChain tools
- ✅ Authentication and permissions preserved
- ✅ Error handling works correctly
- ✅ Audit logging continues to function

---

## Phase 3: Memory & Session Management (Week 2)

### 3.1 Integrate LangChain Memory with Existing Redis

**File**: `src/second_brain_database/integrations/langchain/memory/redis_memory.py`

```python
"""
LangChain memory integration with existing Redis infrastructure.

Uses the existing RedisManager for:
- Conversation history storage
- Session state management
- Message TTL and expiration
"""

from typing import Optional, Dict, Any, List
from datetime import datetime, timezone
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import RedisChatMessageHistory
from langchain.schema import BaseMessage

from ....managers.redis_manager import redis_manager
from ....database import db_manager
from ....managers.logging_manager import get_logger
from ....config import settings

logger = get_logger(prefix="[LangChain_Memory]")


class MCPIntegratedMemory:
    """
    LangChain memory system integrated with existing Redis infrastructure.
    
    Features:
    - Uses existing RedisManager for storage
    - Supports conversation buffer and entity memory
    - Persists to MongoDB for long-term storage
    - Follows existing TTL patterns
    """
    
    def __init__(self, user_id: str, session_id: str, agent_type: str):
        self.user_id = user_id
        self.session_id = session_id
        self.agent_type = agent_type
        
        # Use existing Redis manager
        redis_url = redis_manager.redis_url
        
        # Create LangChain memory with Redis backend
        self.chat_history = RedisChatMessageHistory(
            session_id=f"ai_session_{user_id}_{session_id}",
            url=redis_url,
            ttl=settings.LANGCHAIN_MEMORY_TTL
        )
        
        self.memory = ConversationBufferMemory(
            chat_memory=self.chat_history,
            return_messages=True,
            memory_key="chat_history",
            max_token_limit=settings.LANGCHAIN_MAX_TOKENS
        )
        
        logger.info(
            f"Initialized LangChain memory for user {user_id}, "
            f"session {session_id}, agent {agent_type}"
        )
    
    async def add_message(self, role: str, content: str) -> None:
        """Add message to conversation history."""
        try:
            if role == "user":
                self.memory.chat_memory.add_user_message(content)
            else:
                self.memory.chat_memory.add_ai_message(content)
            
            logger.debug(
                f"Added {role} message to session {self.session_id}",
                extra={"user_id": self.user_id}
            )
        except Exception as e:
            logger.error(f"Error adding message to memory: {e}", exc_info=True)
    
    async def get_messages(self, limit: Optional[int] = None) -> List[BaseMessage]:
        """Get conversation history."""
        try:
            messages = self.memory.chat_memory.messages
            if limit:
                return messages[-limit:]
            return messages
        except Exception as e:
            logger.error(f"Error retrieving messages: {e}", exc_info=True)
            return []
    
    async def save_to_mongodb(self) -> None:
        """
        Persist conversation to MongoDB for long-term storage.
        
        Uses existing DatabaseManager patterns.
        """
        try:
            collection = db_manager.get_collection("ai_conversations")
            
            messages = await self.get_messages()
            
            conversation_doc = {
                "user_id": self.user_id,
                "session_id": self.session_id,
                "agent_type": self.agent_type,
                "messages": [
                    {
                        "role": msg.type,
                        "content": msg.content,
                        "timestamp": datetime.now(timezone.utc)
                    }
                    for msg in messages
                ],
                "created_at": datetime.now(timezone.utc),
                "updated_at": datetime.now(timezone.utc),
                "total_messages": len(messages)
            }
            
            await collection.insert_one(conversation_doc)
            
            logger.info(
                f"Saved conversation to MongoDB for session {self.session_id}",
                extra={"user_id": self.user_id, "message_count": len(messages)}
            )
        except Exception as e:
            logger.error(
                f"Error saving conversation to MongoDB: {e}",
                exc_info=True
            )
    
    async def clear(self) -> None:
        """Clear conversation history."""
        try:
            self.memory.chat_memory.clear()
            logger.info(f"Cleared memory for session {self.session_id}")
        except Exception as e:
            logger.error(f"Error clearing memory: {e}", exc_info=True)
```

**Validation**:
- ✅ Memory stored in existing Redis instance
- ✅ TTL configuration matches existing patterns
- ✅ MongoDB persistence works correctly
- ✅ No memory leaks or performance degradation

---

## Phase 4: Agent Orchestrator (Week 2-3)

### 4.1 Core Orchestrator Implementation

**File**: `src/second_brain_database/integrations/langchain/orchestrator.py`

```python
"""
LangChain Agent Orchestrator integrated with existing infrastructure.

Coordinates:
- Agent selection and routing
- Tool execution via MCP wrappers
- Memory management via Redis
- Authentication via MCPUserContext
- Rate limiting via SecurityManager
"""

from typing import Dict, Any, Optional, List
from datetime import datetime, timezone
import uuid

from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langchain.schema import BaseMessage

from ...managers.family_manager import family_manager
from ...managers.security_manager import security_manager
from ...managers.redis_manager import redis_manager
from ...database import db_manager
from ...websocket_manager import manager as websocket_manager
from ...managers.logging_manager import get_logger
from ...config import settings

from ...integrations.mcp.context import MCPUserContext, set_mcp_user_context
from .tools.family_tools import get_all_family_tools
from .tools.auth_tools import get_all_auth_tools
from .tools.shop_tools import get_all_shop_tools
from .tools.workspace_tools import get_all_workspace_tools
from .memory.redis_memory import MCPIntegratedMemory

logger = get_logger(prefix="[LangChain_Orchestrator]")


class LangChainOrchestrator:
    """
    Central orchestrator for LangChain agent system.
    
    Integrates with existing:
    - FamilyManager, SecurityManager, RedisManager
    - MCPUserContext for authentication
    - WebSocket manager for streaming
    - Ollama for LLM inference
    """
    
    def __init__(self):
        # Use existing manager instances
        self.family_manager = family_manager
        self.security_manager = security_manager
        self.redis_manager = redis_manager
        self.db_manager = db_manager
        self.websocket_manager = websocket_manager
        
        # Initialize LLM using existing Ollama
        self.llm = ChatOllama(
            base_url=settings.OLLAMA_HOST,
            model=settings.LANGCHAIN_DEFAULT_MODEL,
            temperature=settings.LANGCHAIN_TEMPERATURE
        )
        
        # Load all available tools
        self.all_tools = self._load_all_tools()
        
        logger.info(
            f"LangChain Orchestrator initialized with {len(self.all_tools)} tools"
        )
    
    def _load_all_tools(self) -> List:
        """Load all MCP tool wrappers."""
        tools = []
        tools.extend(get_all_family_tools())
        tools.extend(get_all_auth_tools())
        tools.extend(get_all_shop_tools())
        tools.extend(get_all_workspace_tools())
        # Add other tool categories as needed
        return tools
    
    async def create_session(
        self,
        user_context: MCPUserContext,
        agent_type: str
    ) -> str:
        """
        Create AI session using existing patterns.
        
        Stores session in Redis using existing RedisManager.
        """
        try:
            # Check rate limits using existing SecurityManager
            user_id = user_context.user_id
            
            # Validate concurrent session limit
            redis = await self.redis_manager.get_redis()
            existing_sessions = await redis.keys(f"ai_session_{user_id}_*")
            
            if len(existing_sessions) >= settings.LANGCHAIN_MAX_CONCURRENT_SESSIONS:
                raise ValueError(
                    f"Maximum concurrent sessions ({settings.LANGCHAIN_MAX_CONCURRENT_SESSIONS}) reached"
                )
            
            # Create new session
            session_id = str(uuid.uuid4())
            
            session_data = {
                "session_id": session_id,
                "user_id": user_id,
                "agent_type": agent_type,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "status": "active",
                "message_count": 0
            }
            
            # Store in Redis with TTL
            await redis.setex(
                f"ai_session_{user_id}_{session_id}",
                settings.LANGCHAIN_MEMORY_TTL,
                json.dumps(session_data)
            )
            
            logger.info(
                f"Created AI session {session_id} for user {user_id}, "
                f"agent type: {agent_type}"
            )
            
            return session_id
            
        except Exception as e:
            logger.error(f"Error creating AI session: {e}", exc_info=True)
            raise
    
    async def process_message(
        self,
        user_context: MCPUserContext,
        session_id: str,
        message: str,
        agent_type: str = "personal"
    ) -> Dict[str, Any]:
        """
        Process user message through appropriate agent.
        
        Sets MCPUserContext for tool execution, manages memory,
        and streams responses via WebSocket.
        """
        try:
            # Set user context for MCP tools
            set_mcp_user_context(user_context)
            
            # Initialize memory for session
            memory = MCPIntegratedMemory(
                user_id=user_context.user_id,
                session_id=session_id,
                agent_type=agent_type
            )
            
            # Add user message to memory
            await memory.add_message("user", message)
            
            # Select tools based on agent type
            tools = self._select_tools_for_agent(agent_type)
            
            # Create prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", self._get_system_prompt(agent_type)),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad")
            ])
            
            # Create agent with selected tools
            agent = create_openai_functions_agent(self.llm, tools, prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                memory=memory.memory,
                verbose=True,
                max_iterations=5
            )
            
            # Execute agent
            response = await agent_executor.ainvoke({"input": message})
            
            # Add assistant response to memory
            await memory.add_message("assistant", response["output"])
            
            # Stream response via WebSocket
            if self.websocket_manager:
                await self.websocket_manager.send_personal_message(
                    json.dumps({
                        "type": "agent_response",
                        "content": response["output"],
                        "agent_type": agent_type
                    }),
                    user_context.user_id
                )
            
            # Save conversation to MongoDB
            await memory.save_to_mongodb()
            
            return {
                "content": response["output"],
                "agent_type": agent_type,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            return {
                "error": str(e),
                "agent_type": agent_type
            }
    
    def _select_tools_for_agent(self, agent_type: str) -> List:
        """Select appropriate tools based on agent type."""
        if agent_type == "family":
            return get_all_family_tools()
        elif agent_type == "personal":
            return get_all_auth_tools()
        elif agent_type == "workspace":
            return get_all_workspace_tools()
        elif agent_type == "commerce":
            return get_all_shop_tools()
        else:
            # Default: all tools
            return self.all_tools
    
    def _get_system_prompt(self, agent_type: str) -> str:
        """Get system prompt for agent type."""
        prompts = {
            "family": (
                "You are a helpful family management assistant. "
                "You can help with creating families, inviting members, "
                "managing SBD tokens, and coordinating family activities. "
                "Always be respectful of family privacy and security."
            ),
            "personal": (
                "You are a personal assistant for managing user accounts "
                "and security settings. You can help with profile updates, "
                "2FA setup, password management, and account security."
            ),
            "workspace": (
                "You are a workspace collaboration assistant. "
                "You can help with team management, project coordination, "
                "and workspace administration."
            ),
            "commerce": (
                "You are a shopping assistant. You can help browse items, "
                "make purchases, manage digital assets, and track transactions."
            )
        }
        return prompts.get(agent_type, "You are a helpful AI assistant.")


# Global orchestrator instance
langchain_orchestrator = LangChainOrchestrator()
```

---

## Phase 5: FastAPI Routes Integration (Week 3)

### 5.1 Add AI Routes Following Existing Patterns

**File**: `src/second_brain_database/routes/langgraph/routes.py`

```python
"""
FastAPI routes for LangChain/LangGraph AI features.

Follows existing route patterns:
- Uses APIRouter with prefix
- Integrates get_current_user dependency
- Applies rate limiting via SecurityManager
- Uses error handling decorators
- Implements WebSocket streaming
"""

from fastapi import APIRouter, Depends, HTTPException, WebSocket, WebSocketDisconnect, status
from fastapi.responses import JSONResponse
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field
from datetime import datetime

from ...routes.auth.services.auth.login import get_current_user
from ...managers.security_manager import security_manager
from ...managers.logging_manager import get_logger
from ...utils.error_handling import handle_errors
from ...integrations.mcp.context import create_mcp_user_context_from_fastapi_user
from ...integrations.langchain.orchestrator import langchain_orchestrator
from ...websocket_manager import manager as websocket_manager

router = APIRouter(prefix="/ai", tags=["AI Orchestration"])
logger = get_logger(prefix="[AI_Routes]")


# ===== REQUEST/RESPONSE MODELS =====

class CreateSessionRequest(BaseModel):
    """Request to create AI session."""
    agent_type: str = Field(
        default="personal",
        description="Type of AI agent: personal, family, workspace, commerce, security"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "agent_type": "family"
            }
        }


class ChatMessageRequest(BaseModel):
    """Request to send message to AI agent."""
    session_id: str = Field(..., description="AI session ID")
    message: str = Field(..., min_length=1, max_length=10000)
    agent_type: str = Field(default="personal")
    
    class Config:
        json_schema_extra = {
            "example": {
                "session_id": "123e4567-e89b-12d3-a456-426614174000",
                "message": "Create a new family called 'Smith Family'",
                "agent_type": "family"
            }
        }


class ChatMessageResponse(BaseModel):
    """AI agent response."""
    content: str
    agent_type: str
    timestamp: str
    session_id: str
    
    class Config:
        json_schema_extra = {
            "example": {
                "content": "I've created the 'Smith Family' for you...",
                "agent_type": "family",
                "timestamp": "2024-01-15T10:30:00Z",
                "session_id": "123e4567-e89b-12d3-a456-426614174000"
            }
        }


# ===== RATE LIMITING DEPENDENCY =====

async def check_ai_rate_limit(current_user: dict = Depends(get_current_user)):
    """
    Rate limiting for AI operations using existing SecurityManager.
    
    Follows existing rate limiting patterns.
    """
    user_id = str(current_user["_id"])
    
    # Use existing SecurityManager for rate limiting
    is_allowed = await security_manager.check_rate_limit(
        user_id=user_id,
        action="ai_requests",
        limit=settings.LANGCHAIN_RATE_LIMIT_REQUESTS,
        window=3600  # 1 hour
    )
    
    if not is_allowed:
        logger.warning(
            f"AI rate limit exceeded for user {user_id}",
            extra={"user_id": user_id}
        )
        raise HTTPException(
            status_code=429,
            detail="AI rate limit exceeded. Please try again later."
        )
    
    return current_user


# ===== ENDPOINTS =====

@router.post("/sessions", response_model=Dict[str, Any])
@handle_errors  # Use existing error handling
async def create_ai_session(
    request: CreateSessionRequest,
    current_user: dict = Depends(check_ai_rate_limit)
):
    """
    Create new AI agent session.
    
    **Rate Limit**: 100 requests per hour per user
    
    **Authentication**: Required (JWT token)
    
    **Agent Types**:
    - `personal`: User profile and account management
    - `family`: Family creation and management
    - `workspace`: Team collaboration
    - `commerce`: Shopping and transactions
    - `security`: Admin operations (admin only)
    """
    try:
        # Create MCP user context from FastAPI user
        user_context = await create_mcp_user_context_from_fastapi_user(
            current_user,
            ip_address="127.0.0.1",  # Get from request
            user_agent="LangChain-Client"
        )
        
        # Create session using orchestrator
        session_id = await langchain_orchestrator.create_session(
            user_context=user_context,
            agent_type=request.agent_type
        )
        
        logger.info(
            f"Created AI session {session_id} for user {user_context.user_id}",
            extra={"user_id": user_context.user_id, "agent_type": request.agent_type}
        )
        
        return {
            "session_id": session_id,
            "status": "active",
            "agent_type": request.agent_type,
            "created_at": datetime.utcnow().isoformat(),
            "message": f"{request.agent_type.capitalize()} agent session created successfully"
        }
        
    except Exception as e:
        logger.error(f"Error creating AI session: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat", response_model=ChatMessageResponse)
@handle_errors
async def chat_with_agent(
    request: ChatMessageRequest,
    current_user: dict = Depends(check_ai_rate_limit)
):
    """
    Send message to AI agent and get response.
    
    **Rate Limit**: 100 requests per hour per user
    
    **Authentication**: Required (JWT token)
    
    **Example**:
    ```json
    {
        "session_id": "123e4567-e89b-12d3-a456-426614174000",
        "message": "Show me my families",
        "agent_type": "family"
    }
    ```
    """
    try:
        # Create MCP user context
        user_context = await create_mcp_user_context_from_fastapi_user(
            current_user,
            ip_address="127.0.0.1",
            user_agent="LangChain-Client"
        )
        
        # Process message through orchestrator
        response = await langchain_orchestrator.process_message(
            user_context=user_context,
            session_id=request.session_id,
            message=request.message,
            agent_type=request.agent_type
        )
        
        logger.info(
            f"Processed AI message for session {request.session_id}",
            extra={
                "user_id": user_context.user_id,
                "agent_type": request.agent_type
            }
        )
        
        return ChatMessageResponse(
            content=response["content"],
            agent_type=response["agent_type"],
            timestamp=response["timestamp"],
            session_id=request.session_id
        )
        
    except Exception as e:
        logger.error(f"Error processing AI message: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.websocket("/ws/stream/{session_id}")
async def websocket_ai_stream(
    websocket: WebSocket,
    session_id: str,
    token: str
):
    """
    WebSocket endpoint for streaming AI responses.
    
    Uses existing WebSocket manager patterns.
    
    **Authentication**: JWT token via query parameter
    
    **Message Format**:
    ```json
    {
        "type": "message",
        "content": "Your message here",
        "agent_type": "family"
    }
    ```
    
    **Response Format**:
    ```json
    {
        "type": "agent_response",
        "content": "AI response",
        "agent_type": "family",
        "streaming": true
    }
    ```
    """
    await websocket.accept()
    
    try:
        # Authenticate user from token
        # (Use existing get_current_user logic)
        current_user = await get_current_user(token)
        
        user_context = await create_mcp_user_context_from_fastapi_user(
            current_user,
            ip_address="127.0.0.1",
            user_agent="WebSocket-Client"
        )
        
        logger.info(
            f"WebSocket AI stream connected for session {session_id}",
            extra={"user_id": user_context.user_id}
        )
        
        while True:
            # Receive message from client
            data = await websocket.receive_json()
            
            if data["type"] == "message":
                # Process through orchestrator with streaming
                response = await langchain_orchestrator.process_message(
                    user_context=user_context,
                    session_id=session_id,
                    message=data["content"],
                    agent_type=data.get("agent_type", "personal")
                )
                
                # Send response back
                await websocket.send_json({
                    "type": "agent_response",
                    "content": response["content"],
                    "agent_type": response["agent_type"],
                    "timestamp": response["timestamp"]
                })
                
    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for session {session_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}", exc_info=True)
        await websocket.close()


@router.get("/sessions/{session_id}", response_model=Dict[str, Any])
@handle_errors
async def get_session_info(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Get information about an AI session.
    
    Returns session status, message count, and metadata.
    """
    try:
        user_id = str(current_user["_id"])
        
        # Get session from Redis
        redis = await langchain_orchestrator.redis_manager.get_redis()
        session_data = await redis.get(f"ai_session_{user_id}_{session_id}")
        
        if not session_data:
            raise HTTPException(status_code=404, detail="Session not found")
        
        return json.loads(session_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting session info: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/sessions/{session_id}")
@handle_errors
async def delete_session(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    End an AI session and clear its memory.
    
    Deletes session from Redis and optionally archives to MongoDB.
    """
    try:
        user_id = str(current_user["_id"])
        
        # Delete from Redis
        redis = await langchain_orchestrator.redis_manager.get_redis()
        await redis.delete(f"ai_session_{user_id}_{session_id}")
        
        logger.info(
            f"Deleted AI session {session_id}",
            extra={"user_id": user_id}
        )
        
        return {
            "message": "Session ended successfully",
            "session_id": session_id
        }
        
    except Exception as e:
        logger.error(f"Error deleting session: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
```

**Register Routes** in `src/second_brain_database/main.py`:

```python
# Add to existing route registrations
from .routes.langgraph import routes as ai_routes

# Include AI routes
app.include_router(ai_routes.router, prefix="/api/v1")
```

---

## Testing Strategy

### Unit Tests (`tests/test_langchain/`)

```python
# tests/test_langchain/test_mcp_tool_wrappers.py
import pytest
from src.second_brain_database.integrations.langchain.tools.family_tools import get_family_info
from src.second_brain_database.integrations.mcp.context import MCPUserContext

@pytest.mark.asyncio
async def test_family_tool_wrapper_authentication():
    """Test that MCP tool wrapper preserves authentication."""
    # Should raise authentication error without context
    with pytest.raises(Exception):
        await get_family_info._arun(family_id="test_id")

@pytest.mark.asyncio
async def test_family_tool_wrapper_execution(mock_user_context):
    """Test successful tool execution with valid context."""
    result = await get_family_info._arun(family_id="valid_family_id")
    assert "error" not in result
```

### Integration Tests

```python
# tests/test_langchain/test_orchestrator.py
import pytest
from src.second_brain_database.integrations.langchain.orchestrator import langchain_orchestrator
from src.second_brain_database.integrations.mcp.context import MCPUserContext

@pytest.mark.asyncio
async def test_session_creation(test_user_context):
    """Test AI session creation."""
    session_id = await langchain_orchestrator.create_session(
        user_context=test_user_context,
        agent_type="family"
    )
    assert session_id is not None
    assert len(session_id) == 36  # UUID format

@pytest.mark.asyncio
async def test_message_processing(test_user_context, test_session_id):
    """Test message processing through agent."""
    response = await langchain_orchestrator.process_message(
        user_context=test_user_context,
        session_id=test_session_id,
        message="Show me my families",
        agent_type="family"
    )
    assert "content" in response
    assert response["agent_type"] == "family"
```

### End-to-End Tests

```python
# tests/test_langchain/test_api_routes.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_create_session_endpoint(test_client, auth_token):
    """Test session creation endpoint."""
    response = await test_client.post(
        "/api/v1/ai/sessions",
        json={"agent_type": "family"},
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    assert response.status_code == 200
    assert "session_id" in response.json()

@pytest.mark.asyncio
async def test_chat_endpoint_rate_limiting(test_client, auth_token):
    """Test rate limiting on chat endpoint."""
    # Send 101 requests (exceeding 100/hour limit)
    for i in range(101):
        response = await test_client.post(
            "/api/v1/ai/chat",
            json={
                "session_id": "test_session",
                "message": f"Test message {i}",
                "agent_type": "family"
            },
            headers={"Authorization": f"Bearer {auth_token}"}
        )
        
        if i < 100:
            assert response.status_code == 200
        else:
            assert response.status_code == 429  # Rate limit exceeded
```

---

## Deployment Configuration

### Docker Compose Updates

```yaml
# docker-compose.yml additions
services:
  # Existing services...
  
  langchain:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - LANGCHAIN_ENABLED=true
      - LANGCHAIN_MODEL_PROVIDER=ollama
      - LANGCHAIN_DEFAULT_MODEL=llama3.1
      - LANGCHAIN_TEMPERATURE=0.7
      - LANGCHAIN_MAX_TOKENS=2000
      - LANGCHAIN_MEMORY_TTL=3600
      - OLLAMA_HOST=${OLLAMA_HOST}
      - MONGODB_URL=${MONGODB_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - mongodb
      - redis
      - ollama
    networks:
      - sbd-network
```

---

## Success Metrics

### Technical Metrics
- ✅ **Response Time**: < 300ms for first token
- ✅ **Concurrent Sessions**: 100+ supported per server
- ✅ **Uptime**: 99.9% availability
- ✅ **Error Rate**: < 1% of interactions
- ✅ **Memory Usage**: < 2GB per 1000 sessions

### Business Metrics
- ✅ **User Engagement**: 50% increase in feature usage
- ✅ **Task Completion**: 80% success rate for complex workflows
- ✅ **User Satisfaction**: 4.5+ rating for AI interactions
- ✅ **Family Adoption**: 60% of families using AI features

---

## Migration Timeline

### Week 1: Foundation
- Install dependencies
- Configure settings
- Create base tool wrapper system

### Week 2: Core Integration
- Complete MCP tool wrappers
- Implement memory system
- Build orchestrator

### Week 3: API & Features
- Add FastAPI routes
- Implement specialized agents
- Add WebSocket streaming

### Week 4: Advanced Features
- LangGraph workflows
- Voice integration
- Performance optimization

### Week 5: Testing & Polish
- Write comprehensive tests
- Performance tuning
- Documentation

### Week 6: Production Deployment
- Deploy to staging
- Load testing
- Production rollout

---

## Risk Mitigation

### Technical Risks
1. **LLM Performance**: Monitor response times, implement caching
2. **Memory Leaks**: Use memory profiling, implement cleanup routines
3. **Rate Limiting**: Test under load, adjust limits dynamically
4. **Tool Failures**: Implement fallback mechanisms, graceful degradation

### Business Risks
1. **User Adoption**: Gradual rollout, collect feedback early
2. **Cost Overruns**: Monitor resource usage, implement budgets
3. **Security Issues**: Comprehensive security review, penetration testing
4. **Performance Impact**: Canary deployments, easy rollback

---

## Next Steps

1. **Review and Approve Plan**: Team review and stakeholder sign-off
2. **Set Up Development Environment**: Configure local LangChain setup
3. **Create Feature Branch**: `feature/langchain-integration`
4. **Begin Phase 1**: Install dependencies and configure settings
5. **Daily Stand-ups**: Track progress, address blockers
6. **Weekly Reviews**: Demo progress, gather feedback

---

## Conclusion

This plan provides a comprehensive roadmap for integrating LangChain/LangGraph with your existing Second Brain Database infrastructure. The implementation:

- **Preserves all existing functionality**
- **Leverages current managers and infrastructure**
- **Maintains security and authentication patterns**
- **Follows established code patterns**
- **Provides natural language interface to 138+ existing tools**
- **Enables voice interactions**
- **Supports complex multi-step workflows**
- **Scales horizontally for production loads**

The modular approach allows for incremental development and testing, ensuring stability throughout the integration process.
