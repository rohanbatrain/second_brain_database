{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540b8120",
   "metadata": {},
   "source": [
    "# üß† Second Brain Database - Repository Cleanup & Refactoring Analysis\n",
    "\n",
    "This comprehensive notebook analyzes your mature production codebase and provides automated tools to safely reorganize, clean, and modernize the repository structure while preserving all institutional knowledge.\n",
    "\n",
    "## üìã Executive Summary\n",
    "\n",
    "Your Second Brain Database repository shows:\n",
    "- **492+ tracked files** across 7 major categories\n",
    "- **Rich integration ecosystem** (MCP, LangGraph, N8N, Voice, Auth)\n",
    "- **Comprehensive test suite** (100+ test files)\n",
    "- **Extensive documentation** (80+ markdown files)\n",
    "- **Production-ready infrastructure** with Docker, scripts, and monitoring\n",
    "\n",
    "## üéØ Goals\n",
    "1. **Preserve everything** - no data loss, only reorganization\n",
    "2. **Improve maintainability** - clear folder structure and documentation\n",
    "3. **Enhance developer experience** - easy navigation and contribution\n",
    "4. **Prepare for scale** - production-ready organization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ad556",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Repository Structure Analysis\n",
    "\n",
    "Let's start by analyzing your current repository structure and understanding the complexity we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d05347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Repository root path\n",
    "REPO_ROOT = Path('/Users/rohan/Documents/repos/second_brain_database')\n",
    "\n",
    "# Load the file index\n",
    "with open(REPO_ROOT / 'file_index.txt', 'r') as f:\n",
    "    file_lines = [line.strip().lstrip('./') for line in f if line.strip()]\n",
    "\n",
    "print(f\"üìä Repository Analysis\")\n",
    "print(f\"Total files tracked: {len(file_lines)}\")\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"\\nFirst 10 files:\")\n",
    "for i, file in enumerate(file_lines[:10]):\n",
    "    print(f\"  {i+1:2d}. {file}\")\n",
    "\n",
    "# Basic file extension analysis\n",
    "extensions = Counter()\n",
    "for file in file_lines:\n",
    "    if '.' in file:\n",
    "        ext = '.' + file.split('.')[-1]\n",
    "        extensions[ext] += 1\n",
    "    else:\n",
    "        extensions['no_extension'] += 1\n",
    "\n",
    "print(f\"\\nüìà File types distribution:\")\n",
    "for ext, count in extensions.most_common(10):\n",
    "    print(f\"  {ext:<15} {count:>3d} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e76301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze directory structure depth and organization\n",
    "def analyze_directory_structure(files: List[str]) -> Dict:\n",
    "    \"\"\"Analyze the directory structure of files\"\"\"\n",
    "    structure = defaultdict(list)\n",
    "    depth_analysis = defaultdict(int)\n",
    "    \n",
    "    for file in files:\n",
    "        parts = file.split('/')\n",
    "        depth = len(parts) - 1\n",
    "        depth_analysis[depth] += 1\n",
    "        \n",
    "        if depth > 0:\n",
    "            top_dir = parts[0]\n",
    "            structure[top_dir].append(file)\n",
    "    \n",
    "    return dict(structure), dict(depth_analysis)\n",
    "\n",
    "directories, depth_dist = analyze_directory_structure(file_lines)\n",
    "\n",
    "# Create visualization of directory structure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Directory distribution\n",
    "dir_counts = {k: len(v) for k, v in directories.items()}\n",
    "top_dirs = dict(sorted(dir_counts.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "ax1.bar(top_dirs.keys(), top_dirs.values(), alpha=0.7)\n",
    "ax1.set_title('Files by Top-Level Directory')\n",
    "ax1.set_ylabel('Number of Files')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Depth distribution\n",
    "ax2.bar(depth_dist.keys(), depth_dist.values(), alpha=0.7, color='orange')\n",
    "ax2.set_title('Files by Directory Depth')\n",
    "ax2.set_xlabel('Directory Depth')\n",
    "ax2.set_ylabel('Number of Files')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìÅ Top-level directories analysis:\")\n",
    "for dir_name, file_count in sorted(dir_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {dir_name:<25} {file_count:>3d} files\")\n",
    "\n",
    "print(f\"\\nüìè Directory depth distribution:\")\n",
    "for depth, count in sorted(depth_dist.items()):\n",
    "    indent = \"  \" + \"  \" * depth\n",
    "    print(f\"{indent}Depth {depth}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b7afb",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Code Organization Assessment\n",
    "\n",
    "Now let's categorize files by their purpose and identify organizational patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File categorization rules based on patterns and locations\n",
    "categorization_rules = {\n",
    "    'Core Application Code': {\n",
    "        'patterns': [r'^src/second_brain_database/.*\\.py$'],\n",
    "        'description': 'Production FastAPI application code'\n",
    "    },\n",
    "    'Test Files': {\n",
    "        'patterns': [r'^tests/.*\\.py$', r'test_.*\\.py$', r'.*test.*\\.py$'],\n",
    "        'description': 'Unit and integration tests'\n",
    "    },\n",
    "    'Documentation': {\n",
    "        'patterns': [r'.*\\.md$', r'README.*', r'.*\\.rst$'],\n",
    "        'description': 'Markdown and documentation files'\n",
    "    },\n",
    "    'Scripts & Tools': {\n",
    "        'patterns': [r'^scripts/.*\\.py$', r'.*_script\\.py$'],\n",
    "        'description': 'Development and automation scripts'\n",
    "    },\n",
    "    'Configuration': {\n",
    "        'patterns': [r'.*\\.yml$', r'.*\\.yaml$', r'.*\\.toml$', r'.*\\.json$', r'.*\\.ini$', r'.*\\.env$'],\n",
    "        'description': 'Configuration files'\n",
    "    },\n",
    "    'Infrastructure': {\n",
    "        'patterns': [r'Dockerfile.*', r'docker-compose.*', r'.*\\.dockerfile$'],\n",
    "        'description': 'Docker and deployment files'\n",
    "    },\n",
    "    'Maintenance Scripts': {\n",
    "        'patterns': [r'^(fix_|verify_|clear_|install_|update_|check_).*\\.py$'],\n",
    "        'description': 'One-off maintenance and verification scripts'\n",
    "    },\n",
    "    'Specifications': {\n",
    "        'patterns': [r'^\\.kiro/.*', r'.*spec.*\\.md$', r'.*requirements.*\\.md$'],\n",
    "        'description': 'Product specs and requirements'\n",
    "    },\n",
    "    'Workflows': {\n",
    "        'patterns': [r'^n8n_workflows/.*', r'.*workflow.*\\.md$'],\n",
    "        'description': 'N8N and automation workflows'\n",
    "    },\n",
    "    'Planning Documents': {\n",
    "        'patterns': [r'^TODOS/.*', r'.*_plan.*\\.md$'],\n",
    "        'description': 'Project planning and TODO documents'\n",
    "    }\n",
    "}\n",
    "\n",
    "def categorize_file(filename: str) -> str:\n",
    "    \"\"\"Categorize a file based on patterns\"\"\"\n",
    "    for category, rules in categorization_rules.items():\n",
    "        for pattern in rules['patterns']:\n",
    "            if re.match(pattern, filename, re.IGNORECASE):\n",
    "                return category\n",
    "    return 'Other'\n",
    "\n",
    "# Categorize all files\n",
    "file_categories = defaultdict(list)\n",
    "for file in file_lines:\n",
    "    category = categorize_file(file)\n",
    "    file_categories[category].append(file)\n",
    "\n",
    "# Create summary\n",
    "category_summary = {cat: len(files) for cat, files in file_categories.items()}\n",
    "\n",
    "print(\"üìÇ File Categorization Summary:\")\n",
    "print(\"=\" * 50)\n",
    "total_categorized = 0\n",
    "for category, count in sorted(category_summary.items(), key=lambda x: x[1], reverse=True):\n",
    "    description = categorization_rules.get(category, {}).get('description', 'Miscellaneous files')\n",
    "    print(f\"{category:<25} {count:>3d} files - {description}\")\n",
    "    total_categorized += count\n",
    "\n",
    "print(f\"\\nTotal files categorized: {total_categorized}/{len(file_lines)}\")\n",
    "\n",
    "# Visualize categories\n",
    "plt.figure(figsize=(12, 8))\n",
    "categories = list(category_summary.keys())\n",
    "counts = list(category_summary.values())\n",
    "\n",
    "plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('File Distribution by Category')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of specific problem areas\n",
    "print(\"\\nüîç Detailed Category Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze maintenance scripts (potential cleanup candidates)\n",
    "maintenance_files = file_categories.get('Maintenance Scripts', [])\n",
    "print(f\"\\nüîß Maintenance Scripts ({len(maintenance_files)} files):\")\n",
    "for file in sorted(maintenance_files)[:10]:  # Show first 10\n",
    "    print(f\"  ‚Ä¢ {file}\")\n",
    "if len(maintenance_files) > 10:\n",
    "    print(f\"  ... and {len(maintenance_files) - 10} more\")\n",
    "\n",
    "# Analyze root-level files (should be minimal)\n",
    "root_level_files = [f for f in file_lines if '/' not in f]\n",
    "print(f\"\\nüìÅ Root-level files ({len(root_level_files)} files):\")\n",
    "for file in sorted(root_level_files)[:15]:\n",
    "    print(f\"  ‚Ä¢ {file}\")\n",
    "\n",
    "# Analyze documentation spread\n",
    "doc_files = file_categories.get('Documentation', [])\n",
    "doc_by_location = defaultdict(list)\n",
    "for doc in doc_files:\n",
    "    if '/' in doc:\n",
    "        location = doc.split('/')[0]\n",
    "    else:\n",
    "        location = 'root'\n",
    "    doc_by_location[location].append(doc)\n",
    "\n",
    "print(f\"\\nüìö Documentation distribution ({len(doc_files)} files):\")\n",
    "for location, docs in sorted(doc_by_location.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"  {location:<20} {len(docs):>3d} files\")\n",
    "\n",
    "# Look for potential duplicates or similar files\n",
    "def find_similar_files(files: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Find files with similar names that might be duplicates\"\"\"\n",
    "    similar = []\n",
    "    for i, file1 in enumerate(files):\n",
    "        name1 = os.path.basename(file1).lower()\n",
    "        for file2 in files[i+1:]:\n",
    "            name2 = os.path.basename(file2).lower()\n",
    "            if name1 == name2 and file1 != file2:\n",
    "                similar.append((file1, file2))\n",
    "    return similar\n",
    "\n",
    "similar_files = find_similar_files(file_lines)\n",
    "if similar_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Potential duplicate names ({len(similar_files)} pairs):\")\n",
    "    for file1, file2 in similar_files[:5]:\n",
    "        print(f\"  ‚Ä¢ {file1} ‚Üî {file2}\")\n",
    "    if len(similar_files) > 5:\n",
    "        print(f\"  ... and {len(similar_files) - 5} more pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d262e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Documentation Audit and Consolidation\n",
    "\n",
    "Let's analyze the documentation structure and identify consolidation opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation analysis and consolidation opportunities\n",
    "doc_files = file_categories.get('Documentation', [])\n",
    "\n",
    "# Categorize documentation by topic/theme\n",
    "doc_themes = {\n",
    "    'MCP Integration': [f for f in doc_files if 'mcp' in f.lower()],\n",
    "    'Family Management': [f for f in doc_files if 'family' in f.lower()],\n",
    "    'Production/Deployment': [f for f in doc_files if any(kw in f.lower() for kw in ['production', 'deployment', 'setup'])],\n",
    "    'Authentication': [f for f in doc_files if any(kw in f.lower() for kw in ['auth', 'webauthn', 'token'])],\n",
    "    'Flutter Integration': [f for f in doc_files if 'flutter' in f.lower()],\n",
    "    'Voice/AI': [f for f in doc_files if any(kw in f.lower() for kw in ['voice', 'ai', 'langgraph', 'ollama'])],\n",
    "    'Workflows': [f for f in doc_files if any(kw in f.lower() for kw in ['n8n', 'workflow'])],\n",
    "    'Testing': [f for f in doc_files if 'test' in f.lower()],\n",
    "}\n",
    "\n",
    "print(\"üìö Documentation Themes Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_themed = 0\n",
    "for theme, files in doc_themes.items():\n",
    "    if files:\n",
    "        print(f\"\\n{theme} ({len(files)} files):\")\n",
    "        total_themed += len(files)\n",
    "        for file in sorted(files)[:5]:  # Show first 5\n",
    "            print(f\"  ‚Ä¢ {file}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"  ... and {len(files) - 5} more\")\n",
    "\n",
    "unthemed = [f for f in doc_files if not any(f in theme_files for theme_files in doc_themes.values())]\n",
    "print(f\"\\nUnthemed documentation ({len(unthemed)} files):\")\n",
    "for file in sorted(unthemed)[:10]:\n",
    "    print(f\"  ‚Ä¢ {file}\")\n",
    "\n",
    "print(f\"\\nTotal: {total_themed} themed + {len(unthemed)} unthemed = {total_themed + len(unthemed)} docs\")\n",
    "\n",
    "# Identify potential consolidation opportunities\n",
    "consolidation_opportunities = []\n",
    "\n",
    "# Look for similar documentation files that could be merged\n",
    "def find_consolidation_candidates(files: List[str], theme: str) -> List[Dict]:\n",
    "    \"\"\"Find files that could potentially be consolidated\"\"\"\n",
    "    candidates = []\n",
    "    if len(files) > 3:  # Only suggest consolidation if there are multiple files\n",
    "        candidates.append({\n",
    "            'theme': theme,\n",
    "            'files': files,\n",
    "            'suggestion': f\"Consider consolidating {len(files)} {theme.lower()} docs into a comprehensive guide\"\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "print(f\"\\nüîÑ Consolidation Opportunities:\")\n",
    "print(\"=\" * 50)\n",
    "for theme, files in doc_themes.items():\n",
    "    candidates = find_consolidation_candidates(files, theme)\n",
    "    consolidation_opportunities.extend(candidates)\n",
    "\n",
    "for opp in consolidation_opportunities:\n",
    "    print(f\"\\n{opp['theme']}:\")\n",
    "    print(f\"  {opp['suggestion']}\")\n",
    "    print(f\"  Files to consider: {len(opp['files'])}\")\n",
    "\n",
    "# Visualize documentation distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Theme distribution\n",
    "theme_counts = {k: len(v) for k, v in doc_themes.items() if v}\n",
    "ax1.barh(list(theme_counts.keys()), list(theme_counts.values()))\n",
    "ax1.set_title('Documentation by Theme')\n",
    "ax1.set_xlabel('Number of Files')\n",
    "\n",
    "# Location distribution\n",
    "ax2.pie(doc_by_location.values(), labels=doc_by_location.keys(), autopct='%1.1f%%')\n",
    "ax2.set_title('Documentation by Location')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d804509",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Test Coverage and Quality Review\n",
    "\n",
    "Now let's analyze the test suite structure and identify any gaps or organizational issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b81dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file analysis\n",
    "test_files = file_categories.get('Test Files', [])\n",
    "\n",
    "# Categorize tests by component/feature\n",
    "test_categories = {\n",
    "    'WebAuthn Tests': [f for f in test_files if 'webauthn' in f.lower()],\n",
    "    'Family Management Tests': [f for f in test_files if 'family' in f.lower()],\n",
    "    'MCP Integration Tests': [f for f in test_files if 'mcp' in f.lower()],\n",
    "    'Authentication Tests': [f for f in test_files if any(kw in f.lower() for kw in ['auth', 'token', 'login'])],\n",
    "    'Database Tests': [f for f in test_files if any(kw in f.lower() for kw in ['database', 'db'])],\n",
    "    'Integration Tests': [f for f in test_files if 'integration' in f.lower()],\n",
    "    'Performance Tests': [f for f in test_files if 'performance' in f.lower()],\n",
    "    'Voice/AI Tests': [f for f in test_files if any(kw in f.lower() for kw in ['voice', 'ai'])],\n",
    "}\n",
    "\n",
    "print(\"üß™ Test Suite Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total test files: {len(test_files)}\")\n",
    "\n",
    "total_categorized_tests = 0\n",
    "for category, tests in test_categories.items():\n",
    "    if tests:\n",
    "        print(f\"\\n{category} ({len(tests)} files):\")\n",
    "        total_categorized_tests += len(tests)\n",
    "        for test in sorted(tests)[:3]:  # Show first 3\n",
    "            print(f\"  ‚Ä¢ {test}\")\n",
    "        if len(tests) > 3:\n",
    "            print(f\"  ... and {len(tests) - 3} more\")\n",
    "\n",
    "uncategorized_tests = [f for f in test_files if not any(f in cat_tests for cat_tests in test_categories.values())]\n",
    "print(f\"\\nUncategorized tests ({len(uncategorized_tests)} files):\")\n",
    "for test in sorted(uncategorized_tests)[:5]:\n",
    "    print(f\"  ‚Ä¢ {test}\")\n",
    "\n",
    "# Test quality indicators\n",
    "print(f\"\\nüìä Test Quality Indicators:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Look for comprehensive test patterns\n",
    "comprehensive_tests = [f for f in test_files if any(kw in f.lower() for kw in ['comprehensive', 'complete', 'end_to_end', 'e2e'])]\n",
    "print(f\"Comprehensive tests: {len(comprehensive_tests)}\")\n",
    "\n",
    "# Look for unit vs integration split\n",
    "unit_tests = [f for f in test_files if 'unit' in f.lower()]\n",
    "integration_tests = [f for f in test_files if 'integration' in f.lower()]\n",
    "print(f\"Unit tests: {len(unit_tests)}\")\n",
    "print(f\"Integration tests: {len(integration_tests)}\")\n",
    "\n",
    "# Look for test utilities\n",
    "test_utils = [f for f in test_files if any(kw in f.lower() for kw in ['conftest', 'utils', 'helper', 'fixture'])]\n",
    "print(f\"Test utilities: {len(test_utils)}\")\n",
    "\n",
    "# Analyze test naming patterns\n",
    "test_naming_patterns = {\n",
    "    'test_': len([f for f in test_files if os.path.basename(f).startswith('test_')]),\n",
    "    'Test classes': len([f for f in test_files if 'Test' in os.path.basename(f)]),\n",
    "    'Interactive': len([f for f in test_files if 'interactive' in f.lower()]),\n",
    "}\n",
    "\n",
    "print(f\"\\nTest naming patterns:\")\n",
    "for pattern, count in test_naming_patterns.items():\n",
    "    print(f\"  {pattern:<20} {count} files\")\n",
    "\n",
    "# Visualize test distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Test categories\n",
    "test_cat_counts = {k: len(v) for k, v in test_categories.items() if v}\n",
    "ax1.barh(list(test_cat_counts.keys()), list(test_cat_counts.values()))\n",
    "ax1.set_title('Test Files by Category')\n",
    "ax1.set_xlabel('Number of Files')\n",
    "\n",
    "# Test types\n",
    "test_types = {\n",
    "    'Unit Tests': len(unit_tests),\n",
    "    'Integration Tests': len(integration_tests),\n",
    "    'Comprehensive Tests': len(comprehensive_tests),\n",
    "    'Other Tests': len(test_files) - len(unit_tests) - len(integration_tests) - len(comprehensive_tests)\n",
    "}\n",
    "ax2.pie(test_types.values(), labels=test_types.keys(), autopct='%1.1f%%')\n",
    "ax2.set_title('Test Types Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61bd1c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Production Readiness Evaluation\n",
    "\n",
    "Let's evaluate the production deployment setup and infrastructure components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production infrastructure analysis\n",
    "infra_files = file_categories.get('Infrastructure', [])\n",
    "config_files = file_categories.get('Configuration', [])\n",
    "scripts_files = file_categories.get('Scripts & Tools', [])\n",
    "\n",
    "print(\"üè≠ Production Infrastructure Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Docker and containerization\n",
    "docker_files = [f for f in file_lines if any(kw in f.lower() for kw in ['dockerfile', 'docker-compose'])]\n",
    "print(f\"Docker files: {len(docker_files)}\")\n",
    "for f in docker_files:\n",
    "    print(f\"  ‚Ä¢ {f}\")\n",
    "\n",
    "# Configuration management\n",
    "config_analysis = {\n",
    "    'JSON configs': [f for f in config_files if f.endswith('.json')],\n",
    "    'YAML configs': [f for f in config_files if f.endswith(('.yml', '.yaml'))],\n",
    "    'TOML configs': [f for f in config_files if f.endswith('.toml')],\n",
    "    'Environment files': [f for f in config_files if '.env' in f],\n",
    "}\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configuration Files ({len(config_files)} total):\")\n",
    "for config_type, files in config_analysis.items():\n",
    "    if files:\n",
    "        print(f\"  {config_type}: {len(files)} files\")\n",
    "        for f in files[:2]:  # Show first 2\n",
    "            print(f\"    - {f}\")\n",
    "        if len(files) > 2:\n",
    "            print(f\"    ... and {len(files) - 2} more\")\n",
    "\n",
    "# Production scripts and automation\n",
    "production_scripts = [f for f in file_lines if any(kw in f.lower() for kw in ['production', 'deploy', 'startup'])]\n",
    "print(f\"\\nüöÄ Production Scripts ({len(production_scripts)} files):\")\n",
    "for script in sorted(production_scripts)[:10]:\n",
    "    print(f\"  ‚Ä¢ {script}\")\n",
    "\n",
    "# Manual operation scripts\n",
    "manual_scripts = [f for f in file_lines if 'manual/' in f]\n",
    "print(f\"\\nüîß Manual Operation Scripts ({len(manual_scripts)} files):\")\n",
    "for script in sorted(manual_scripts):\n",
    "    print(f\"  ‚Ä¢ {script}\")\n",
    "\n",
    "# Health check and monitoring\n",
    "monitoring_files = [f for f in file_lines if any(kw in f.lower() for kw in ['health', 'monitor', 'check', 'status'])]\n",
    "print(f\"\\nüìä Monitoring & Health Check Files ({len(monitoring_files)} files):\")\n",
    "for f in sorted(monitoring_files)[:8]:\n",
    "    print(f\"  ‚Ä¢ {f}\")\n",
    "\n",
    "# Production readiness checklist\n",
    "production_readiness = {\n",
    "    'Docker Setup': len(docker_files) > 0,\n",
    "    'Environment Configuration': len([f for f in config_files if 'env' in f]) > 0,\n",
    "    'Production Scripts': len(production_scripts) > 0,\n",
    "    'Health Monitoring': len([f for f in monitoring_files if 'health' in f.lower()]) > 0,\n",
    "    'Deployment Guides': len([f for f in doc_files if 'deploy' in f.lower()]) > 0,\n",
    "    'Setup Documentation': len([f for f in doc_files if 'setup' in f.lower()]) > 0,\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Production Readiness Checklist:\")\n",
    "print(\"=\" * 40)\n",
    "for item, status in production_readiness.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {item}\")\n",
    "\n",
    "# Identify infrastructure improvements needed\n",
    "improvements = []\n",
    "if len(docker_files) == 1:\n",
    "    improvements.append(\"Consider multi-stage Dockerfile or docker-compose for different environments\")\n",
    "if not any('ci' in f.lower() or 'github' in f.lower() for f in file_lines):\n",
    "    improvements.append(\"Add CI/CD pipeline configuration\")\n",
    "if not any('makefile' in f.lower() for f in file_lines):\n",
    "    improvements.append(\"Consider adding Makefile for common operations\")\n",
    "\n",
    "print(f\"\\nüîß Suggested Infrastructure Improvements:\")\n",
    "for i, improvement in enumerate(improvements, 1):\n",
    "    print(f\"  {i}. {improvement}\")\n",
    "\n",
    "# Visualize production components\n",
    "prod_components = {\n",
    "    'Docker Files': len(docker_files),\n",
    "    'Config Files': len(config_files),\n",
    "    'Production Scripts': len(production_scripts),\n",
    "    'Manual Scripts': len(manual_scripts),\n",
    "    'Monitoring Files': len(monitoring_files),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(prod_components.keys(), prod_components.values(), alpha=0.7, color='skyblue')\n",
    "plt.title('Production Infrastructure Components')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ba7b5",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Refactoring Strategy Development\n",
    "\n",
    "Based on our analysis, let's create a comprehensive refactoring and cleanup strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive refactoring strategy\n",
    "refactoring_strategy = {\n",
    "    'infra/': {\n",
    "        'description': 'Infrastructure and deployment files',\n",
    "        'files': (\n",
    "            docker_files +\n",
    "            [f for f in file_lines if f in ['production_app.py', 'verify_platform_config.py']] +\n",
    "            production_scripts\n",
    "        ),\n",
    "        'priority': 'High'\n",
    "    },\n",
    "    'scripts/maintenance/': {\n",
    "        'description': 'One-off maintenance and fix scripts',\n",
    "        'files': maintenance_files + [\n",
    "            f for f in file_lines if any(pattern in f for pattern in [\n",
    "                'fix_all_indentation.py', 'clear_rate_limits.py', 'install_deepseek.py',\n",
    "                'update_mcp_tools.py', 'check_mcp_health.py', 'verify_task3_implementation.py'\n",
    "            ])\n",
    "        ],\n",
    "        'priority': 'Medium'\n",
    "    },\n",
    "    'scripts/tools/': {\n",
    "        'description': 'Development and utility scripts',\n",
    "        'files': [f for f in scripts_files if 'manual/' not in f],\n",
    "        'priority': 'Medium'\n",
    "    },\n",
    "    'automation/': {\n",
    "        'description': 'N8N workflows and automation',\n",
    "        'files': [f for f in file_lines if f.startswith('n8n_workflows/')],\n",
    "        'priority': 'Medium'\n",
    "    },\n",
    "    'docs/production/': {\n",
    "        'description': 'Production deployment and setup guides',\n",
    "        'files': [f for f in doc_files if any(kw in f.lower() for kw in [\n",
    "            'production', 'deployment', 'setup', 'startup'\n",
    "        ])],\n",
    "        'priority': 'High'\n",
    "    },\n",
    "    'docs/integrations/mcp/': {\n",
    "        'description': 'MCP integration documentation',\n",
    "        'files': doc_themes.get('MCP Integration', []),\n",
    "        'priority': 'High'\n",
    "    },\n",
    "    'docs/integrations/family/': {\n",
    "        'description': 'Family management system documentation',\n",
    "        'files': doc_themes.get('Family Management', []),\n",
    "        'priority': 'High'\n",
    "    },\n",
    "    'docs/integrations/auth/': {\n",
    "        'description': 'Authentication and security documentation',\n",
    "        'files': doc_themes.get('Authentication', []),\n",
    "        'priority': 'High'\n",
    "    },\n",
    "    'docs/integrations/voice/': {\n",
    "        'description': 'Voice and AI integration documentation',\n",
    "        'files': doc_themes.get('Voice/AI', []),\n",
    "        'priority': 'Medium'\n",
    "    },\n",
    "    'docs/specs/': {\n",
    "        'description': 'Product specifications and requirements',\n",
    "        'files': [f for f in file_lines if f.startswith('.kiro/')],\n",
    "        'priority': 'Medium'\n",
    "    },\n",
    "    'docs/plans/': {\n",
    "        'description': 'Project planning and TODO documents',\n",
    "        'files': [f for f in file_lines if f.startswith('TODOS/')],\n",
    "        'priority': 'Low'\n",
    "    },\n",
    "    'legacy/': {\n",
    "        'description': 'Legacy and experimental files',\n",
    "        'files': [\n",
    "            f for f in file_lines if any(kw in f.lower() for kw in [\n",
    "                'unused', 'old', 'deprecated', 'backup'\n",
    "            ])\n",
    "        ] + [\n",
    "            f for f in root_level_files if f not in [\n",
    "                'README.md', 'Dockerfile', 'requirements.txt', 'pyproject.toml',\n",
    "                'docker-compose.yml', 'QUICKSTART.md', 'SETUP_GUIDE.md'\n",
    "            ]\n",
    "        ],\n",
    "        'priority': 'Low'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate migration impact\n",
    "print(\"üìã Refactoring Strategy Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_files_to_move = 0\n",
    "for destination, info in refactoring_strategy.items():\n",
    "    files_count = len(info['files'])\n",
    "    total_files_to_move += files_count\n",
    "    priority_icon = {\"High\": \"üî¥\", \"Medium\": \"üü°\", \"Low\": \"üü¢\"}.get(info['priority'], \"‚ö™\")\n",
    "    \n",
    "    print(f\"\\n{priority_icon} {destination} ({files_count} files)\")\n",
    "    print(f\"  Priority: {info['priority']}\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    \n",
    "    # Show sample files\n",
    "    sample_files = info['files'][:3]\n",
    "    for f in sample_files:\n",
    "        print(f\"    ‚Ä¢ {f}\")\n",
    "    if len(info['files']) > 3:\n",
    "        print(f\"    ... and {len(info['files']) - 3} more\")\n",
    "\n",
    "print(f\"\\nTotal files to relocate: {total_files_to_move}/{len(file_lines)} ({total_files_to_move/len(file_lines)*100:.1f}%)\")\n",
    "print(f\"Files staying in place: {len(file_lines) - total_files_to_move}\")\n",
    "\n",
    "# Create priority-based migration phases\n",
    "migration_phases = {\n",
    "    'Phase 1 - Critical Infrastructure': [dest for dest, info in refactoring_strategy.items() if info['priority'] == 'High'],\n",
    "    'Phase 2 - Development Tools': [dest for dest, info in refactoring_strategy.items() if info['priority'] == 'Medium'],\n",
    "    'Phase 3 - Cleanup & Archive': [dest for dest, info in refactoring_strategy.items() if info['priority'] == 'Low']\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ Migration Phases:\")\n",
    "print(\"=\" * 30)\n",
    "for phase, destinations in migration_phases.items():\n",
    "    files_in_phase = sum(len(refactoring_strategy[dest]['files']) for dest in destinations)\n",
    "    print(f\"\\n{phase} ({files_in_phase} files):\")\n",
    "    for dest in destinations:\n",
    "        print(f\"  ‚Ä¢ {dest}\")\n",
    "\n",
    "# Visualize refactoring impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Files by destination\n",
    "dest_counts = {dest: len(info['files']) for dest, info in refactoring_strategy.items() if info['files']}\n",
    "ax1.barh(list(dest_counts.keys()), list(dest_counts.values()))\n",
    "ax1.set_title('Files by Destination Directory')\n",
    "ax1.set_xlabel('Number of Files')\n",
    "\n",
    "# Priority distribution\n",
    "priority_counts = defaultdict(int)\n",
    "for info in refactoring_strategy.values():\n",
    "    priority_counts[info['priority']] += len(info['files'])\n",
    "\n",
    "colors = {'High': 'red', 'Medium': 'orange', 'Low': 'green'}\n",
    "ax2.pie(priority_counts.values(), labels=priority_counts.keys(), \n",
    "        colors=[colors[p] for p in priority_counts.keys()], autopct='%1.1f%%')\n",
    "ax2.set_title('Migration Priority Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead754e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Cleanup Automation Implementation\n",
    "\n",
    "Now let's create automated tools to safely execute the refactoring plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9da7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate automated cleanup script\n",
    "def generate_cleanup_script():\n",
    "    \"\"\"Generate a comprehensive cleanup script\"\"\"\n",
    "    \n",
    "    script_content = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Second Brain Database Repository Cleanup Script\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "This script safely reorganizes the repository structure while preserving all files.\n",
    "Run with --dry-run first to see what changes will be made.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Repository root\n",
    "REPO_ROOT = Path(__file__).parent\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create the new directory structure\"\"\"\n",
    "    directories = [\n",
    "        \"infra\",\n",
    "        \"scripts/maintenance\",\n",
    "        \"scripts/tools\", \n",
    "        \"automation\",\n",
    "        \"docs/production\",\n",
    "        \"docs/integrations/mcp\",\n",
    "        \"docs/integrations/family\",\n",
    "        \"docs/integrations/auth\", \n",
    "        \"docs/integrations/voice\",\n",
    "        \"docs/specs\",\n",
    "        \"docs/plans\",\n",
    "        \"legacy\"\n",
    "    ]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = REPO_ROOT / dir_path\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úÖ Created directory: {{dir_path}}\")\n",
    "\n",
    "def move_file_safely(src: str, dst_dir: str, dry_run: bool = False):\n",
    "    \"\"\"Safely move a file to destination directory\"\"\"\n",
    "    src_path = REPO_ROOT / src\n",
    "    dst_path = REPO_ROOT / dst_dir / os.path.basename(src)\n",
    "    \n",
    "    if not src_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Source file does not exist: {{src}}\")\n",
    "        return False\n",
    "        \n",
    "    if dst_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Destination already exists: {{dst_path}}\")\n",
    "        return False\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"üîÑ Would move: {{src}} ‚Üí {{dst_dir}}/{{os.path.basename(src)}}\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(str(src_path), str(dst_path))\n",
    "        print(f\"‚úÖ Moved: {{src}} ‚Üí {{dst_dir}}/{{os.path.basename(src)}}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error moving {{src}}: {{e}}\")\n",
    "        return False\n",
    "\n",
    "# File migration mappings\n",
    "MIGRATION_MAP = {{\n",
    "'''\n",
    "\n",
    "    # Add the migration mappings\n",
    "    for destination, info in refactoring_strategy.items():\n",
    "        if info['files']:\n",
    "            script_content += f'    \"{destination}\": [\\n'\n",
    "            for file_path in info['files']:\n",
    "                script_content += f'        \"{file_path}\",\\n'\n",
    "            script_content += '    ],\\n'\n",
    "    \n",
    "    script_content += '''\n",
    "}\n",
    "\n",
    "def create_cleanup_log(moves_made):\n",
    "    \"\"\"Create a log of all moves made\"\"\"\n",
    "    log_content = f\"\"\"# Repository Cleanup Log\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Summary\n",
    "- Total files moved: {len(moves_made)}\n",
    "- New directory structure created\n",
    "\n",
    "## File Moves\n",
    "\"\"\"\n",
    "    \n",
    "    for src, dst in moves_made:\n",
    "        log_content += f\"- `{src}` ‚Üí `{dst}`\\\\n\"\n",
    "    \n",
    "    with open(REPO_ROOT / \"CLEANUP_LOG.md\", \"w\") as f:\n",
    "        f.write(log_content)\n",
    "    \n",
    "    print(f\"üìã Created cleanup log: CLEANUP_LOG.md\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Cleanup Second Brain Database repository\")\n",
    "    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Show what would be done without making changes\")\n",
    "    parser.add_argument(\"--phase\", choices=[\"1\", \"2\", \"3\", \"all\"], default=\"all\", help=\"Run specific migration phase\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"üß† Second Brain Database Repository Cleanup\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if args.dry_run:\n",
    "        print(\"üîç DRY RUN MODE - No files will be moved\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    if not args.dry_run:\n",
    "        create_directory_structure()\n",
    "    \n",
    "    # Execute migrations\n",
    "    moves_made = []\n",
    "    total_moves = 0\n",
    "    successful_moves = 0\n",
    "    \n",
    "    phase_map = {\n",
    "        \"1\": [\"docs/production/\", \"docs/integrations/mcp/\", \"docs/integrations/family/\", \"docs/integrations/auth/\", \"infra/\"],\n",
    "        \"2\": [\"scripts/maintenance/\", \"scripts/tools/\", \"automation/\", \"docs/integrations/voice/\", \"docs/specs/\"],\n",
    "        \"3\": [\"docs/plans/\", \"legacy/\"]\n",
    "    }\n",
    "    \n",
    "    phases_to_run = phase_map.get(args.phase, []) if args.phase != \"all\" else list(MIGRATION_MAP.keys())\n",
    "    \n",
    "    for destination in phases_to_run:\n",
    "        if destination in MIGRATION_MAP:\n",
    "            print(f\"\\\\nüìÇ Processing: {destination}\")\n",
    "            for file_path in MIGRATION_MAP[destination]:\n",
    "                total_moves += 1\n",
    "                if move_file_safely(file_path, destination, args.dry_run):\n",
    "                    successful_moves += 1\n",
    "                    if not args.dry_run:\n",
    "                        moves_made.append((file_path, destination))\n",
    "    \n",
    "    print(f\"\\\\nüìä Summary:\")\n",
    "    print(f\"Total files processed: {total_moves}\")\n",
    "    print(f\"Successful moves: {successful_moves}\")\n",
    "    print(f\"Failed moves: {total_moves - successful_moves}\")\n",
    "    \n",
    "    if not args.dry_run and moves_made:\n",
    "        create_cleanup_log(moves_made)\n",
    "        print(f\"\\\\nüéâ Repository cleanup completed!\")\n",
    "        print(f\"Review CLEANUP_LOG.md for details of all changes made.\")\n",
    "    elif args.dry_run:\n",
    "        print(f\"\\\\nüîç Dry run completed. Use --phase 1,2,3 or remove --dry-run to execute.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    return script_content\n",
    "\n",
    "# Create the cleanup script\n",
    "cleanup_script = generate_cleanup_script()\n",
    "\n",
    "# Save the script\n",
    "script_path = REPO_ROOT / \"cleanup_repository.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(cleanup_script)\n",
    "\n",
    "# Make it executable\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(\"üõ†Ô∏è Generated cleanup automation script:\")\n",
    "print(f\"Script location: {script_path}\")\n",
    "print(\"\\nüìã Usage Instructions:\")\n",
    "print(\"1. First run with dry-run to preview changes:\")\n",
    "print(\"   python cleanup_repository.py --dry-run\")\n",
    "print(\"\\n2. Run by phases for safety:\")\n",
    "print(\"   python cleanup_repository.py --phase 1  # Critical infrastructure\")\n",
    "print(\"   python cleanup_repository.py --phase 2  # Development tools\") \n",
    "print(\"   python cleanup_repository.py --phase 3  # Cleanup & archive\")\n",
    "print(\"\\n3. Or run all phases at once:\")\n",
    "print(\"   python cleanup_repository.py\")\n",
    "\n",
    "print(f\"\\nüìä Script will move {sum(len(info['files']) for info in refactoring_strategy.values())} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e57a4",
   "metadata": {},
   "source": [
    "## üéØ Next Steps & Execution Plan\n",
    "\n",
    "### Phase 1: Analysis & Validation (Safe)\n",
    "1. **Run this notebook** to generate comprehensive repository analysis\n",
    "2. **Review refactoring strategy** and file categorizations \n",
    "3. **Test cleanup script** with `--dry-run` flag\n",
    "4. **Validate documentation** consolidation plans\n",
    "\n",
    "### Phase 2: Gradual Migration (Cautious)\n",
    "1. **Backup repository** (create git branch: `git checkout -b pre-cleanup-backup`)\n",
    "2. **Run Phase 1** migrations (infrastructure & documentation)\n",
    "3. **Test builds** to ensure nothing breaks\n",
    "4. **Run Phase 2** migrations (development tools & scripts)\n",
    "\n",
    "### Phase 3: Finalization (Confident)\n",
    "1. **Run Phase 3** migrations (cleanup & archive)\n",
    "2. **Update CI/CD** references to moved files\n",
    "3. **Update documentation** with new structure\n",
    "4. **Create team announcement** with migration guide\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Generated Automation Tools\n",
    "\n",
    "This notebook creates several automated tools:\n",
    "\n",
    "1. **`cleanup_repository.py`** - Main migration script with dry-run capability\n",
    "2. **File categorization analysis** - Automated classification of all repository files  \n",
    "3. **Documentation consolidation strategy** - Merge similar docs, preserve all content\n",
    "4. **Test coverage analysis** - Identify gaps and reorganization opportunities\n",
    "5. **Production readiness assessment** - Validate critical deployment files\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Safety Features\n",
    "\n",
    "- **Dry-run mode**: Preview all changes before execution\n",
    "- **Phased migration**: Run in 3 safe phases with validation points\n",
    "- **Complete logging**: Track every file move with detailed logs\n",
    "- **Rollback capability**: Git-based rollback if issues arise\n",
    "- **Zero data loss**: All files preserved, only reorganized\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Team Communication\n",
    "\n",
    "**Before running cleanup:**\n",
    "1. Share this analysis with your team\n",
    "2. Get approval for the refactoring strategy\n",
    "3. Schedule the migration during low-activity periods\n",
    "4. Ensure all team members have backed up local work\n",
    "\n",
    "**After cleanup:**\n",
    "1. Update team documentation with new structure\n",
    "2. Share CLEANUP_LOG.md for transparency  \n",
    "3. Update development workflows and tooling\n",
    "4. Celebrate your clean, modern repository! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
