# LangChain Quick Reference

> **Quick commands and tips for working with LangChain + LangSmith in Second Brain Database**

---

## üöÄ Quick Start

### Enable LangSmith Tracing (Recommended)

```bash
# 1. Get API key from https://smith.langchain.com
# 2. Add to .env or .sbd:
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=lsv2_pt_your_key_here
export LANGCHAIN_PROJECT=SecondBrainDatabase

# 3. Restart application
```

### Test the Integration

```bash
# Run full coverage test
python3 final_coverage_test.py

# Should show:
# ‚úÖ Total Tools Loaded: 33
# ‚úÖ FULL COVERAGE: Workspace (27/27) + Admin (19/19) = 46/46 tools
```

---

## üìù Configuration Cheat Sheet

### Essential Environment Variables

```bash
# LangChain Core
LANGCHAIN_ENABLED=true
LANGCHAIN_MODEL_PROVIDER=ollama
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b
LANGCHAIN_TEMPERATURE=0.7
LANGCHAIN_MAX_TOKENS=2048

# Memory
LANGCHAIN_MEMORY_TTL=3600  # 1 hour
LANGCHAIN_CONVERSATION_HISTORY_LIMIT=50

# LangSmith (Optional but Recommended)
LANGCHAIN_TRACING_V2=false  # Set to 'true' to enable
LANGCHAIN_PROJECT=SecondBrainDatabase
LANGCHAIN_API_KEY=  # Your API key

# Ollama
OLLAMA_HOST=http://127.0.0.1:11434

# Redis (for memory)
REDIS_HOST=127.0.0.1
REDIS_PORT=6379
REDIS_DB=0
```

### Quick Model Switching

```bash
# Small & Fast (default)
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b

# Reasoning Models
LANGCHAIN_DEFAULT_MODEL=deepseek-r1:1.5b

# Larger Models (slower but better)
LANGCHAIN_DEFAULT_MODEL=llama3.2:3b
LANGCHAIN_DEFAULT_MODEL=qwen2.5:7b
```

---

## üîß Common Commands

### Check What Models Are Available

```bash
# List installed Ollama models
ollama list

# Pull a new model
ollama pull llama3.2:1b
ollama pull deepseek-r1:1.5b
```

### Test Individual Components

```python
# Test orchestrator creation
python3 -c "
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager

settings = Settings()
redis_manager = RedisManager()
orchestrator = LangChainOrchestrator(settings, redis_manager)
print(f'‚úÖ Orchestrator ready with model: {settings.LANGCHAIN_DEFAULT_MODEL}')
"

# Test tool creation
python3 -c "
from src.second_brain_database.integrations.mcp.context import MCPUserContext
from src.second_brain_database.integrations.langchain.tools.family_tools import create_family_tools

user_context = MCPUserContext(
    user_id='test',
    username='test',
    role='admin',
    permissions=['admin', 'family:read']
)

tools = create_family_tools(user_context)
print(f'‚úÖ Created {len(tools)} family tools')
for tool in tools:
    print(f'  - {tool.name}')
"
```

### View Conversation History

```bash
# Connect to Redis
redis-cli

# List all chat sessions
KEYS chat_history:*

# View a specific session
GET chat_history:session_123

# Clear a session
DEL chat_history:session_123

# Clear all sessions
FLUSHDB
```

---

## üêõ Debugging

### Enable Verbose Logging

```python
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Check Tool Availability

```python
import asyncio
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager
from src.second_brain_database.integrations.mcp.context import MCPUserContext

async def check_tools():
    settings = Settings()
    redis_manager = RedisManager()
    orchestrator = LangChainOrchestrator(settings, redis_manager)
    
    user_context = MCPUserContext(
        user_id="test",
        username="test",
        role="admin",
        permissions=["admin"]
    )
    
    tools = orchestrator._create_tools_for_user(user_context)
    
    print(f"Total tools: {len(tools)}")
    for tool in tools:
        print(f"  - {tool.name}: {tool.description[:50]}...")

asyncio.run(check_tools())
```

### Test Agent Response

```python
import asyncio
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager
from src.second_brain_database.integrations.mcp.context import MCPUserContext

async def test_chat():
    settings = Settings()
    redis_manager = RedisManager()
    orchestrator = LangChainOrchestrator(settings, redis_manager)
    
    user_context = MCPUserContext(
        user_id="test_user",
        username="test",
        role="admin",
        permissions=["admin", "family:read"]
    )
    
    response = await orchestrator.chat(
        session_id="test_session",
        user_id="test_user",
        message="What tools can you use?",
        user_context=user_context
    )
    
    print(f"Response: {response['response']}")

asyncio.run(test_chat())
```

---

## üìä Monitoring with LangSmith

### Access LangSmith Dashboard

1. Go to [https://smith.langchain.com](https://smith.langchain.com)
2. Navigate to your project: `SecondBrainDatabase`
3. View traces, metrics, and performance

### What to Monitor

| Metric | Good | Warning | Critical |
|--------|------|---------|----------|
| Response Time | < 2s | 2-5s | > 5s |
| Error Rate | < 1% | 1-5% | > 5% |
| Token Usage | < 1000/req | 1000-2000 | > 2000 |
| Tool Errors | 0% | < 1% | > 1% |

### Creating Test Datasets

```python
# In LangSmith UI:
# 1. Create dataset: "test-cases"
# 2. Add examples:
#    - Input: "What families am I in?"
#    - Expected: Contains family information
# 3. Run evaluations against dataset
# 4. Compare model/prompt variations
```

---

## üîç Troubleshooting

### Issue: Agent Returns Empty Response

**Check:**
1. Is Ollama running? `curl http://127.0.0.1:11434`
2. Is model downloaded? `ollama list`
3. Is Redis running? `redis-cli ping`

**Fix:**
```bash
# Start Ollama
ollama serve &

# Pull model
ollama pull llama3.2:1b

# Start Redis
redis-server --daemonize yes
```

### Issue: "Tool already exists" Warnings

**Fixed!** We removed duplicates:
- Disabled `resources_registration.py`
- Disabled `prompts_registration.py`
- Removed duplicate `backup_admin` functions

Verify fix:
```bash
python3 final_coverage_test.py 2>&1 | grep -i "already exists"
# Should return nothing
```

### Issue: Out of Memory

**Causes:**
- Too many conversation history messages
- Too many concurrent sessions

**Fix:**
```bash
# Reduce history limit
LANGCHAIN_CONVERSATION_HISTORY_LIMIT=20  # Down from 50

# Reduce TTL
LANGCHAIN_MEMORY_TTL=1800  # 30 minutes instead of 1 hour

# Clear old sessions
redis-cli KEYS "chat_history:*" | xargs redis-cli DEL
```

### Issue: Slow Response Times

**Optimize:**
```bash
# Use smaller model
LANGCHAIN_DEFAULT_MODEL=llama3.2:1b  # Fastest

# Reduce max tokens
LANGCHAIN_MAX_TOKENS=1024  # Down from 2048

# Lower temperature (less creative, faster)
LANGCHAIN_TEMPERATURE=0.3  # Down from 0.7
```

---

## üìà Performance Tips

### Agent Caching

Already implemented! Agents are cached per user:

```python
# Cached agent key: f"{user_id}:{agent_type}"
# Reused across requests
```

### Memory Trimming

Already implemented! History auto-trims to 50 messages:

```python
if len(history.messages) > 50:
    history.messages = history.messages[-50:]
```

### Redis Connection Pooling

Already implemented! Redis manager uses connection pool.

### Parallel Tool Execution

Not yet implemented. Agent executes tools sequentially.

**To enable parallel execution**, consider LangGraph's built-in parallelization:
```python
# Future enhancement
from langgraph.graph import StateGraph
# Define parallel tool execution nodes
```

---

## üéØ Quick Tests

### Test 1: Tool Count

```bash
python3 -c "
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager
from src.second_brain_database.integrations.mcp.context import MCPUserContext

settings = Settings()
redis_manager = RedisManager()
orchestrator = LangChainOrchestrator(settings, redis_manager)

# Admin user
admin_context = MCPUserContext(
    user_id='admin',
    username='admin',
    role='admin',
    permissions=['admin']
)
admin_tools = orchestrator._create_tools_for_user(admin_context)
print(f'Admin tools: {len(admin_tools)} (should be 33)')

# Regular user
user_context = MCPUserContext(
    user_id='user',
    username='user',
    role='user',
    permissions=['family:read', 'shop:read']
)
user_tools = orchestrator._create_tools_for_user(user_context)
print(f'User tools: {len(user_tools)} (should be less than 33)')
"
```

### Test 2: Memory Persistence

```bash
# Terminal 1: Create session
python3 -c "
import asyncio
from src.second_brain_database.integrations.langchain.orchestrator import LangChainOrchestrator
from src.second_brain_database.config import Settings
from src.second_brain_database.managers.redis_manager import RedisManager
from src.second_brain_database.integrations.mcp.context import MCPUserContext

async def test():
    settings = Settings()
    redis_manager = RedisManager()
    orchestrator = LangChainOrchestrator(settings, redis_manager)
    
    user_context = MCPUserContext(
        user_id='test',
        username='test',
        role='admin',
        permissions=['admin']
    )
    
    # Send message
    await orchestrator.chat(
        session_id='test_session',
        user_id='test',
        message='Remember: my favorite color is blue',
        user_context=user_context
    )
    print('‚úÖ Message sent')

asyncio.run(test())
"

# Terminal 2: Check Redis
redis-cli GET chat_history:test_session
# Should show the conversation
```

### Test 3: LangSmith Tracing

```bash
# 1. Enable tracing
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=lsv2_pt_your_key

# 2. Run test
python3 final_coverage_test.py

# 3. Visit https://smith.langchain.com/SecondBrainDatabase
# Should see trace for the test run
```

---

## üìö Useful Links

| Resource | URL |
|----------|-----|
| **LangSmith Dashboard** | https://smith.langchain.com |
| **LangChain Docs** | https://python.langchain.com/docs |
| **LangGraph Docs** | https://langchain-ai.github.io/langgraph |
| **Ollama Models** | https://ollama.com/library |
| **Our Documentation** | [LANGCHAIN_LANGSMITH_GUIDE.md](./LANGCHAIN_LANGSMITH_GUIDE.md) |
| **Architecture Diagrams** | [LANGCHAIN_ARCHITECTURE.md](./LANGCHAIN_ARCHITECTURE.md) |

---

## üéì Learning Path

1. **Understand the Flow**
   - Read [LANGCHAIN_ARCHITECTURE.md](./LANGCHAIN_ARCHITECTURE.md)
   - Visualize the request flow

2. **Run Tests**
   - Execute `python3 final_coverage_test.py`
   - Verify all 33 tools load

3. **Enable LangSmith**
   - Get API key
   - Set environment variables
   - View your first trace

4. **Experiment**
   - Try different models
   - Adjust temperature
   - Compare results in LangSmith

5. **Optimize**
   - Monitor performance
   - Tune based on metrics
   - Create test datasets

---

**Last Updated:** 2 November 2025  
**Maintainer:** Second Brain Database Team
