"""
RAG Monitoring & Observability System

Comprehensive monitoring system for RAG operations including metrics collection,
performance tracking, health checks, alerting, and observability dashboards
for production deployment and system optimization.
"""

import asyncio
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import statistics
import time
from typing import Any, Callable, Dict, List, Optional, Tuple

import psutil

from second_brain_database.managers.logging_manager import get_logger
from second_brain_database.managers.redis_manager import redis_manager
from second_brain_database.rag.core.exceptions import RAGError
from second_brain_database.rag.core.types import DocumentChunk, QueryRequest, QueryResponse

logger = get_logger()


class MetricType(str, Enum):
    """Types of metrics collected."""
    COUNTER = "counter"          # Incrementing counter
    GAUGE = "gauge"             # Current value
    HISTOGRAM = "histogram"      # Distribution of values
    TIMER = "timer"             # Duration measurements
    RATE = "rate"               # Rate of events


class AlertSeverity(str, Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class HealthStatus(str, Enum):
    """System health status."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    DOWN = "down"


@dataclass
class Metric:
    """A metric data point."""
    name: str
    metric_type: MetricType
    value: float
    timestamp: datetime
    labels: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Alert:
    """An alert generated by the monitoring system."""
    alert_id: str
    name: str
    severity: AlertSeverity
    message: str
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    labels: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class HealthCheck:
    """A health check result."""
    component: str
    status: HealthStatus
    message: str
    timestamp: datetime
    response_time_ms: float
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceProfile:
    """Performance profile for a component."""
    component: str
    avg_response_time_ms: float
    p95_response_time_ms: float
    p99_response_time_ms: float
    throughput_per_second: float
    error_rate: float
    success_rate: float
    total_requests: int
    memory_usage_mb: float
    cpu_usage_percent: float


class MetricsCollector:
    """
    Metrics collection system for RAG operations.
    
    Collects, aggregates, and stores performance metrics with
    support for different metric types and time-series data.
    """
    
    def __init__(self, retention_hours: int = 24):
        """
        Initialize metrics collector.
        
        Args:
            retention_hours: How long to retain metrics data
        """
        self.retention_hours = retention_hours
        self.metrics_store: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.counters: Dict[str, float] = defaultdict(float)
        self.gauges: Dict[str, float] = {}
        
        logger.info("Initialized metrics collector")
    
    async def record_metric(
        self,
        name: str,
        metric_type: MetricType,
        value: float,
        labels: Optional[Dict[str, str]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Record a metric."""
        try:
            metric = Metric(
                name=name,
                metric_type=metric_type,
                value=value,
                timestamp=datetime.utcnow(),
                labels=labels or {},
                metadata=metadata or {}
            )
            
            # Store in appropriate structure
            if metric_type == MetricType.COUNTER:
                self.counters[name] += value
            elif metric_type == MetricType.GAUGE:
                self.gauges[name] = value
            
            # Store in time-series
            self.metrics_store[name].append(metric)
            
            # Persist to Redis for distributed access
            await self._persist_metric(metric)
            
            logger.debug(f"Recorded metric: {name}={value} ({metric_type})")
            
        except Exception as e:
            logger.error(f"Failed to record metric {name}: {e}")
    
    async def get_metric_history(
        self,
        name: str,
        hours: int = 1,
        labels: Optional[Dict[str, str]] = None
    ) -> List[Metric]:
        """Get metric history for a time period."""
        try:
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            
            metrics = []
            for metric in self.metrics_store[name]:
                if metric.timestamp >= cutoff_time:
                    # Filter by labels if specified
                    if labels:
                        if all(metric.labels.get(k) == v for k, v in labels.items()):
                            metrics.append(metric)
                    else:
                        metrics.append(metric)
            
            return metrics
            
        except Exception as e:
            logger.error(f"Failed to get metric history for {name}: {e}")
            return []
    
    async def get_aggregated_metrics(
        self,
        names: List[str],
        hours: int = 1
    ) -> Dict[str, Dict[str, float]]:
        """Get aggregated statistics for metrics."""
        try:
            result = {}
            
            for name in names:
                history = await self.get_metric_history(name, hours)
                
                if history:
                    values = [m.value for m in history]
                    result[name] = {
                        "count": len(values),
                        "sum": sum(values),
                        "avg": statistics.mean(values),
                        "min": min(values),
                        "max": max(values),
                        "std": statistics.stdev(values) if len(values) > 1 else 0.0,
                        "p50": statistics.median(values),
                        "p95": self._percentile(values, 0.95),
                        "p99": self._percentile(values, 0.99)
                    }
                else:
                    result[name] = {
                        "count": 0,
                        "sum": 0.0,
                        "avg": 0.0,
                        "min": 0.0,
                        "max": 0.0,
                        "std": 0.0,
                        "p50": 0.0,
                        "p95": 0.0,
                        "p99": 0.0
                    }
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to get aggregated metrics: {e}")
            return {}
    
    def _percentile(self, values: List[float], percentile: float) -> float:
        """Calculate percentile of values."""
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        index = int(percentile * (len(sorted_values) - 1))
        return sorted_values[index]
    
    async def _persist_metric(self, metric: Metric):
        """Persist metric to Redis."""
        try:
            key = f"rag:metrics:{metric.name}:{metric.timestamp.strftime('%Y%m%d%H')}"
            
            metric_data = {
                "name": metric.name,
                "type": metric.metric_type,
                "value": metric.value,
                "timestamp": metric.timestamp.isoformat(),
                "labels": metric.labels,
                "metadata": metric.metadata
            }
            
            await redis_manager.lpush(key, json.dumps(metric_data))
            await redis_manager.expire(key, self.retention_hours * 3600)
            
        except Exception as e:
            logger.warning(f"Failed to persist metric: {e}")


class PerformanceTracker:
    """
    Performance tracking system for RAG components.
    
    Tracks response times, throughput, resource usage, and
    generates performance profiles for optimization.
    """
    
    def __init__(self, metrics_collector: MetricsCollector):
        """
        Initialize performance tracker.
        
        Args:
            metrics_collector: Metrics collector instance
        """
        self.metrics = metrics_collector
        self.active_requests: Dict[str, float] = {}
        self.component_stats: Dict[str, Dict[str, Any]] = defaultdict(dict)
        
        logger.info("Initialized performance tracker")
    
    async def start_request_tracking(self, request_id: str, component: str) -> str:
        """Start tracking a request."""
        start_time = time.time()
        self.active_requests[request_id] = start_time
        
        await self.metrics.record_metric(
            f"rag.{component}.requests.started",
            MetricType.COUNTER,
            1.0,
            labels={"component": component}
        )
        
        return request_id
    
    async def end_request_tracking(
        self,
        request_id: str,
        component: str,
        success: bool = True,
        error_type: Optional[str] = None
    ):
        """End tracking a request and record metrics."""
        if request_id not in self.active_requests:
            logger.warning(f"Request {request_id} not found in active requests")
            return
        
        start_time = self.active_requests.pop(request_id)
        duration_ms = (time.time() - start_time) * 1000
        
        # Record response time
        await self.metrics.record_metric(
            f"rag.{component}.response_time_ms",
            MetricType.HISTOGRAM,
            duration_ms,
            labels={"component": component, "success": str(success)}
        )
        
        # Record completion
        await self.metrics.record_metric(
            f"rag.{component}.requests.completed",
            MetricType.COUNTER,
            1.0,
            labels={
                "component": component,
                "success": str(success),
                "error_type": error_type or "none"
            }
        )
        
        # Update component stats
        self._update_component_stats(component, duration_ms, success)
    
    async def track_resource_usage(self):
        """Track system resource usage."""
        try:
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            await self.metrics.record_metric(
                "rag.system.cpu_usage_percent",
                MetricType.GAUGE,
                cpu_percent
            )
            
            # Memory usage
            memory = psutil.virtual_memory()
            await self.metrics.record_metric(
                "rag.system.memory_usage_percent",
                MetricType.GAUGE,
                memory.percent
            )
            
            await self.metrics.record_metric(
                "rag.system.memory_usage_mb",
                MetricType.GAUGE,
                memory.used / (1024 * 1024)
            )
            
            # Disk usage
            disk = psutil.disk_usage('/')
            await self.metrics.record_metric(
                "rag.system.disk_usage_percent",
                MetricType.GAUGE,
                disk.percent
            )
            
        except Exception as e:
            logger.error(f"Failed to track resource usage: {e}")
    
    async def get_performance_profile(self, component: str, hours: int = 1) -> PerformanceProfile:
        """Get performance profile for a component."""
        try:
            # Get metrics for the component
            response_times = await self.metrics.get_metric_history(
                f"rag.{component}.response_time_ms",
                hours
            )
            
            completed_requests = await self.metrics.get_metric_history(
                f"rag.{component}.requests.completed",
                hours
            )
            
            if not response_times or not completed_requests:
                return PerformanceProfile(
                    component=component,
                    avg_response_time_ms=0.0,
                    p95_response_time_ms=0.0,
                    p99_response_time_ms=0.0,
                    throughput_per_second=0.0,
                    error_rate=0.0,
                    success_rate=0.0,
                    total_requests=0,
                    memory_usage_mb=0.0,
                    cpu_usage_percent=0.0
                )
            
            # Calculate response time statistics
            response_values = [m.value for m in response_times]
            avg_response_time = statistics.mean(response_values)
            p95_response_time = self.metrics._percentile(response_values, 0.95)
            p99_response_time = self.metrics._percentile(response_values, 0.99)
            
            # Calculate success/error rates
            total_requests = len(completed_requests)
            successful_requests = len([
                m for m in completed_requests
                if m.labels.get("success") == "True"
            ])
            
            success_rate = successful_requests / total_requests if total_requests > 0 else 0.0
            error_rate = 1.0 - success_rate
            
            # Calculate throughput
            if completed_requests:
                time_span_hours = hours
                throughput_per_second = total_requests / (time_span_hours * 3600)
            else:
                throughput_per_second = 0.0
            
            # Get current resource usage
            memory_metrics = await self.metrics.get_metric_history("rag.system.memory_usage_mb", 0.1)
            cpu_metrics = await self.metrics.get_metric_history("rag.system.cpu_usage_percent", 0.1)
            
            current_memory = memory_metrics[-1].value if memory_metrics else 0.0
            current_cpu = cpu_metrics[-1].value if cpu_metrics else 0.0
            
            return PerformanceProfile(
                component=component,
                avg_response_time_ms=avg_response_time,
                p95_response_time_ms=p95_response_time,
                p99_response_time_ms=p99_response_time,
                throughput_per_second=throughput_per_second,
                error_rate=error_rate,
                success_rate=success_rate,
                total_requests=total_requests,
                memory_usage_mb=current_memory,
                cpu_usage_percent=current_cpu
            )
            
        except Exception as e:
            logger.error(f"Failed to get performance profile for {component}: {e}")
            return PerformanceProfile(
                component=component,
                avg_response_time_ms=0.0,
                p95_response_time_ms=0.0,
                p99_response_time_ms=0.0,
                throughput_per_second=0.0,
                error_rate=0.0,
                success_rate=0.0,
                total_requests=0,
                memory_usage_mb=0.0,
                cpu_usage_percent=0.0
            )
    
    def _update_component_stats(self, component: str, duration_ms: float, success: bool):
        """Update internal component statistics."""
        stats = self.component_stats[component]
        
        if "response_times" not in stats:
            stats["response_times"] = deque(maxlen=1000)
        if "total_requests" not in stats:
            stats["total_requests"] = 0
        if "successful_requests" not in stats:
            stats["successful_requests"] = 0
        
        stats["response_times"].append(duration_ms)
        stats["total_requests"] += 1
        
        if success:
            stats["successful_requests"] += 1


class HealthCheckManager:
    """
    Health check system for RAG components.
    
    Performs regular health checks on all system components
    and generates health status reports.
    """
    
    def __init__(self, metrics_collector: MetricsCollector):
        """
        Initialize health check manager.
        
        Args:
            metrics_collector: Metrics collector instance
        """
        self.metrics = metrics_collector
        self.health_checks: Dict[str, Callable] = {}
        self.health_status: Dict[str, HealthCheck] = {}
        
        logger.info("Initialized health check manager")
    
    def register_health_check(self, component: str, check_function: Callable):
        """Register a health check function for a component."""
        self.health_checks[component] = check_function
        logger.info(f"Registered health check for component: {component}")
    
    async def run_health_check(self, component: str) -> HealthCheck:
        """Run health check for a specific component."""
        if component not in self.health_checks:
            return HealthCheck(
                component=component,
                status=HealthStatus.UNHEALTHY,
                message="No health check registered",
                timestamp=datetime.utcnow(),
                response_time_ms=0.0
            )
        
        try:
            start_time = time.time()
            
            # Run the health check
            check_result = await self.health_checks[component]()
            
            response_time_ms = (time.time() - start_time) * 1000
            
            # Determine status
            if isinstance(check_result, dict):
                status = HealthStatus(check_result.get("status", "unhealthy"))
                message = check_result.get("message", "Health check completed")
                details = check_result.get("details", {})
            else:
                status = HealthStatus.HEALTHY if check_result else HealthStatus.UNHEALTHY
                message = "Health check completed"
                details = {}
            
            health_check = HealthCheck(
                component=component,
                status=status,
                message=message,
                timestamp=datetime.utcnow(),
                response_time_ms=response_time_ms,
                details=details
            )
            
            self.health_status[component] = health_check
            
            # Record metrics
            await self.metrics.record_metric(
                f"rag.health.{component}.status",
                MetricType.GAUGE,
                1.0 if status == HealthStatus.HEALTHY else 0.0,
                labels={"component": component, "status": status}
            )
            
            await self.metrics.record_metric(
                f"rag.health.{component}.response_time_ms",
                MetricType.HISTOGRAM,
                response_time_ms,
                labels={"component": component}
            )
            
            return health_check
            
        except Exception as e:
            logger.error(f"Health check failed for {component}: {e}")
            
            health_check = HealthCheck(
                component=component,
                status=HealthStatus.UNHEALTHY,
                message=f"Health check failed: {e}",
                timestamp=datetime.utcnow(),
                response_time_ms=0.0,
                details={"error": str(e)}
            )
            
            self.health_status[component] = health_check
            return health_check
    
    async def run_all_health_checks(self) -> Dict[str, HealthCheck]:
        """Run all registered health checks."""
        results = {}
        
        for component in self.health_checks:
            results[component] = await self.run_health_check(component)
        
        return results
    
    async def get_system_health_status(self) -> Dict[str, Any]:
        """Get overall system health status."""
        health_checks = await self.run_all_health_checks()
        
        if not health_checks:
            return {
                "overall_status": HealthStatus.DOWN,
                "message": "No health checks configured",
                "components": {},
                "healthy_count": 0,
                "total_count": 0
            }
        
        healthy_count = sum(
            1 for check in health_checks.values()
            if check.status == HealthStatus.HEALTHY
        )
        
        total_count = len(health_checks)
        
        # Determine overall status
        if healthy_count == total_count:
            overall_status = HealthStatus.HEALTHY
        elif healthy_count > total_count // 2:
            overall_status = HealthStatus.DEGRADED
        elif healthy_count > 0:
            overall_status = HealthStatus.UNHEALTHY
        else:
            overall_status = HealthStatus.DOWN
        
        return {
            "overall_status": overall_status,
            "message": f"{healthy_count}/{total_count} components healthy",
            "components": {
                component: {
                    "status": check.status,
                    "message": check.message,
                    "response_time_ms": check.response_time_ms,
                    "timestamp": check.timestamp.isoformat()
                }
                for component, check in health_checks.items()
            },
            "healthy_count": healthy_count,
            "total_count": total_count
        }


class AlertManager:
    """
    Alert management system for RAG operations.
    
    Monitors metrics and generates alerts based on thresholds
    and anomaly detection algorithms.
    """
    
    def __init__(self, metrics_collector: MetricsCollector):
        """
        Initialize alert manager.
        
        Args:
            metrics_collector: Metrics collector instance
        """
        self.metrics = metrics_collector
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        
        logger.info("Initialized alert manager")
    
    def add_alert_rule(
        self,
        name: str,
        metric_name: str,
        threshold: float,
        comparison: str = "greater_than",  # greater_than, less_than, equals
        severity: AlertSeverity = AlertSeverity.WARNING,
        message_template: str = "Alert triggered for {metric_name}: {value} {comparison} {threshold}"
    ):
        """Add an alert rule."""
        self.alert_rules[name] = {
            "metric_name": metric_name,
            "threshold": threshold,
            "comparison": comparison,
            "severity": severity,
            "message_template": message_template
        }
        
        logger.info(f"Added alert rule: {name}")
    
    async def check_alert_rules(self) -> List[Alert]:
        """Check all alert rules and generate alerts."""
        new_alerts = []
        
        for rule_name, rule in self.alert_rules.items():
            try:
                # Get recent metrics
                metrics = await self.metrics.get_metric_history(
                    rule["metric_name"],
                    hours=0.1  # Last 6 minutes
                )
                
                if not metrics:
                    continue
                
                # Get latest value
                latest_metric = metrics[-1]
                value = latest_metric.value
                threshold = rule["threshold"]
                
                # Check threshold
                triggered = False
                if rule["comparison"] == "greater_than" and value > threshold:
                    triggered = True
                elif rule["comparison"] == "less_than" and value < threshold:
                    triggered = True
                elif rule["comparison"] == "equals" and abs(value - threshold) < 0.001:
                    triggered = True
                
                if triggered:
                    # Check if alert is already active
                    if rule_name not in self.active_alerts:
                        alert = Alert(
                            alert_id=f"{rule_name}_{int(time.time())}",
                            name=rule_name,
                            severity=rule["severity"],
                            message=rule["message_template"].format(
                                metric_name=rule["metric_name"],
                                value=value,
                                comparison=rule["comparison"],
                                threshold=threshold
                            ),
                            timestamp=datetime.utcnow(),
                            labels=latest_metric.labels
                        )
                        
                        self.active_alerts[rule_name] = alert
                        self.alert_history.append(alert)
                        new_alerts.append(alert)
                        
                        logger.warning(f"Alert triggered: {rule_name} - {alert.message}")
                
                else:
                    # Check if we should resolve an active alert
                    if rule_name in self.active_alerts:
                        alert = self.active_alerts[rule_name]
                        alert.resolved = True
                        alert.resolved_at = datetime.utcnow()
                        del self.active_alerts[rule_name]
                        
                        logger.info(f"Alert resolved: {rule_name}")
                
            except Exception as e:
                logger.error(f"Failed to check alert rule {rule_name}: {e}")
        
        return new_alerts
    
    async def get_active_alerts(self) -> List[Alert]:
        """Get all active alerts."""
        return list(self.active_alerts.values())
    
    async def get_alert_history(self, hours: int = 24) -> List[Alert]:
        """Get alert history for a time period."""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        return [
            alert for alert in self.alert_history
            if alert.timestamp >= cutoff_time
        ]


class RAGMonitoringSystem:
    """
    Comprehensive monitoring system for RAG operations.
    
    Integrates metrics collection, performance tracking, health checks,
    and alerting into a unified monitoring solution.
    """
    
    def __init__(
        self,
        metrics_retention_hours: int = 24,
        health_check_interval_seconds: int = 60,
        alert_check_interval_seconds: int = 30
    ):
        """
        Initialize RAG monitoring system.
        
        Args:
            metrics_retention_hours: How long to retain metrics
            health_check_interval_seconds: Health check frequency
            alert_check_interval_seconds: Alert check frequency
        """
        self.metrics = MetricsCollector(metrics_retention_hours)
        self.performance = PerformanceTracker(self.metrics)
        self.health = HealthCheckManager(self.metrics)
        self.alerts = AlertManager(self.metrics)
        
        self.health_check_interval = health_check_interval_seconds
        self.alert_check_interval = alert_check_interval_seconds
        
        self._monitoring_task = None
        self._running = False
        
        # Setup default alert rules
        self._setup_default_alerts()
        
        logger.info("Initialized RAG monitoring system")
    
    async def start_monitoring(self):
        """Start the monitoring system."""
        if self._running:
            return
        
        self._running = True
        self._monitoring_task = asyncio.create_task(self._monitoring_loop())
        
        logger.info("Started RAG monitoring system")
    
    async def stop_monitoring(self):
        """Stop the monitoring system."""
        self._running = False
        
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("Stopped RAG monitoring system")
    
    async def get_dashboard_data(self) -> Dict[str, Any]:
        """Get comprehensive dashboard data."""
        try:
            # Get system health
            health_status = await self.health.get_system_health_status()
            
            # Get performance profiles for key components
            components = ["query_orchestrator", "vector_store", "llm_service", "document_processor"]
            performance_profiles = {}
            
            for component in components:
                performance_profiles[component] = await self.performance.get_performance_profile(component)
            
            # Get recent metrics
            metric_names = [
                "rag.query_orchestrator.response_time_ms",
                "rag.vector_store.search_time_ms",
                "rag.llm_service.generation_time_ms",
                "rag.system.cpu_usage_percent",
                "rag.system.memory_usage_percent"
            ]
            
            metrics_data = await self.metrics.get_aggregated_metrics(metric_names, hours=1)
            
            # Get active alerts
            active_alerts = await self.alerts.get_active_alerts()
            
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "health": health_status,
                "performance": {
                    component: {
                        "avg_response_time_ms": profile.avg_response_time_ms,
                        "p95_response_time_ms": profile.p95_response_time_ms,
                        "throughput_per_second": profile.throughput_per_second,
                        "error_rate": profile.error_rate,
                        "success_rate": profile.success_rate
                    }
                    for component, profile in performance_profiles.items()
                },
                "metrics": metrics_data,
                "alerts": {
                    "active_count": len(active_alerts),
                    "active_alerts": [
                        {
                            "name": alert.name,
                            "severity": alert.severity,
                            "message": alert.message,
                            "timestamp": alert.timestamp.isoformat()
                        }
                        for alert in active_alerts
                    ]
                },
                "system_resources": {
                    "cpu_usage": metrics_data.get("rag.system.cpu_usage_percent", {}).get("avg", 0),
                    "memory_usage": metrics_data.get("rag.system.memory_usage_percent", {}).get("avg", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get dashboard data: {e}")
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e)
            }
    
    def _setup_default_alerts(self):
        """Setup default alert rules."""
        # High response time alerts
        self.alerts.add_alert_rule(
            "high_query_response_time",
            "rag.query_orchestrator.response_time_ms",
            5000.0,  # 5 seconds
            "greater_than",
            AlertSeverity.WARNING,
            "High query response time detected: {value:.1f}ms > {threshold}ms"
        )
        
        # High error rate alerts
        self.alerts.add_alert_rule(
            "high_error_rate",
            "rag.error_rate",
            0.1,  # 10%
            "greater_than",
            AlertSeverity.ERROR,
            "High error rate detected: {value:.1%} > {threshold:.1%}"
        )
        
        # System resource alerts
        self.alerts.add_alert_rule(
            "high_cpu_usage",
            "rag.system.cpu_usage_percent",
            80.0,  # 80%
            "greater_than",
            AlertSeverity.WARNING,
            "High CPU usage: {value:.1f}% > {threshold}%"
        )
        
        self.alerts.add_alert_rule(
            "high_memory_usage",
            "rag.system.memory_usage_percent",
            85.0,  # 85%
            "greater_than",
            AlertSeverity.WARNING,
            "High memory usage: {value:.1f}% > {threshold}%"
        )
    
    async def _monitoring_loop(self):
        """Main monitoring loop."""
        last_health_check = 0
        last_alert_check = 0
        
        while self._running:
            try:
                current_time = time.time()
                
                # Track system resources
                await self.performance.track_resource_usage()
                
                # Run health checks
                if current_time - last_health_check >= self.health_check_interval:
                    await self.health.run_all_health_checks()
                    last_health_check = current_time
                
                # Check alert rules
                if current_time - last_alert_check >= self.alert_check_interval:
                    await self.alerts.check_alert_rules()
                    last_alert_check = current_time
                
                # Sleep for a short interval
                await asyncio.sleep(5)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                await asyncio.sleep(10)  # Wait longer on error


# Global monitoring instance
monitoring_system: Optional[RAGMonitoringSystem] = None


def get_monitoring_system() -> RAGMonitoringSystem:
    """Get the global monitoring system instance."""
    global monitoring_system
    if monitoring_system is None:
        monitoring_system = RAGMonitoringSystem()
    return monitoring_system


# Convenience functions for metric recording
async def record_query_metrics(
    component: str,
    duration_ms: float,
    success: bool,
    chunks_retrieved: int = 0,
    tokens_generated: int = 0
):
    """Record metrics for a query operation."""
    monitoring = get_monitoring_system()
    
    await monitoring.metrics.record_metric(
        f"rag.{component}.response_time_ms",
        MetricType.HISTOGRAM,
        duration_ms,
        labels={"success": str(success)}
    )
    
    if chunks_retrieved > 0:
        await monitoring.metrics.record_metric(
            f"rag.{component}.chunks_retrieved",
            MetricType.HISTOGRAM,
            float(chunks_retrieved)
        )
    
    if tokens_generated > 0:
        await monitoring.metrics.record_metric(
            f"rag.{component}.tokens_generated",
            MetricType.HISTOGRAM,
            float(tokens_generated)
        )


async def record_cache_metrics(hit: bool, cache_level: str):
    """Record cache performance metrics."""
    monitoring = get_monitoring_system()
    
    await monitoring.metrics.record_metric(
        f"rag.cache.{cache_level}.requests",
        MetricType.COUNTER,
        1.0,
        labels={"hit": str(hit)}
    )


async def record_error_metrics(component: str, error_type: str):
    """Record error metrics."""
    monitoring = get_monitoring_system()
    
    await monitoring.metrics.record_metric(
        f"rag.{component}.errors",
        MetricType.COUNTER,
        1.0,
        labels={"error_type": error_type}
    )