"""
AI Orchestration Prometheus Metrics

This module provides Prometheus metrics integration for the AI orchestration system,
extending the existing FastAPI Prometheus instrumentation with AI-specific metrics.
"""

import time
from typing import Dict, Any, Optional
from datetime import datetime, timezone

try:
    from prometheus_client import Counter, Histogram, Gauge, Info
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

from second_brain_database.managers.logging_manager import get_logger

logger = get_logger(prefix="[AI_Metrics]")


class AIPrometheusMetrics:
    """
    Prometheus metrics collector for AI orchestration system.
    
    Integrates with existing Prometheus instrumentation to provide
    AI-specific metrics for monitoring and alerting.
    """
    
    def __init__(self):
        self.enabled = PROMETHEUS_AVAILABLE
        
        if not self.enabled:
            logger.warning("Prometheus client not available - AI metrics disabled")
            return
        
        # Check if metrics are already registered (for testing)
        try:
            from prometheus_client import REGISTRY
            # Try to unregister existing metrics to avoid conflicts in tests
            collectors_to_remove = []
            for collector in list(REGISTRY._collector_to_names.keys()):
                if hasattr(collector, '_name') and collector._name.startswith('ai_'):
                    collectors_to_remove.append(collector)
            
            for collector in collectors_to_remove:
                try:
                    REGISTRY.unregister(collector)
                except KeyError:
                    pass  # Already unregistered
        except Exception:
            pass  # Ignore registry cleanup errors
        
        # Session metrics
        self.ai_sessions_total = Counter(
            'ai_sessions_created_total',
            'Total number of AI sessions created',
            ['agent_type', 'user_id']
        )
        
        self.ai_sessions_active = Gauge(
            'ai_sessions_active_current',
            'Number of currently active AI sessions',
            ['agent_type']
        )
        
        self.ai_session_duration = Histogram(
            'ai_session_duration_seconds',
            'Duration of AI sessions in seconds',
            ['agent_type'],
            buckets=[1, 5, 10, 30, 60, 300, 600, 1800, 3600]
        )
        
        # Message metrics
        self.ai_messages_total = Counter(
            'ai_messages_total',
            'Total number of AI messages processed',
            ['agent_type', 'message_type', 'role']
        )
        
        self.ai_message_processing_duration = Histogram(
            'ai_message_processing_duration_seconds',
            'Time taken to process AI messages',
            ['agent_type'],
            buckets=[0.1, 0.5, 1, 2, 5, 10, 30]
        )
        
        # Model engine metrics
        self.ai_model_requests_total = Counter(
            'ai_model_requests_total',
            'Total number of model inference requests',
            ['model_name', 'status']
        )
        
        self.ai_model_response_time = Histogram(
            'ai_model_response_time_seconds',
            'Model inference response time',
            ['model_name'],
            buckets=[0.1, 0.3, 0.5, 1, 2, 5, 10]
        )
        
        self.ai_model_tokens_generated = Counter(
            'ai_model_tokens_generated_total',
            'Total number of tokens generated by models',
            ['model_name']
        )
        
        # Memory layer metrics
        self.ai_memory_cache_hits = Counter(
            'ai_memory_cache_hits_total',
            'Total number of memory cache hits',
            ['cache_type']
        )
        
        self.ai_memory_cache_misses = Counter(
            'ai_memory_cache_misses_total',
            'Total number of memory cache misses',
            ['cache_type']
        )
        
        self.ai_memory_usage_bytes = Gauge(
            'ai_memory_usage_bytes',
            'Current memory usage in bytes',
            ['component']
        )
        
        # Tool execution metrics
        self.ai_tool_executions_total = Counter(
            'ai_tool_executions_total',
            'Total number of tool executions',
            ['tool_name', 'status']
        )
        
        self.ai_tool_execution_duration = Histogram(
            'ai_tool_execution_duration_seconds',
            'Tool execution duration',
            ['tool_name'],
            buckets=[0.1, 0.5, 1, 2, 5, 10, 30]
        )
        
        # Error metrics
        self.ai_errors_total = Counter(
            'ai_errors_total',
            'Total number of AI system errors',
            ['component', 'error_type']
        )
        
        # Health metrics
        self.ai_health_check_duration = Histogram(
            'ai_health_check_duration_seconds',
            'Duration of AI health checks',
            buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5]
        )
        
        self.ai_component_health = Gauge(
            'ai_component_health',
            'Health status of AI components (1=healthy, 0=unhealthy)',
            ['component']
        )
        
        # System info
        self.ai_system_info = Info(
            'ai_system_info',
            'AI system information'
        )
        
        # Initialize system info
        self.update_system_info()
        
        logger.info("AI Prometheus metrics initialized successfully")
    
    def update_system_info(self):
        """Update AI system information metrics."""
        if not self.enabled:
            return
        
        try:
            from second_brain_database.config import settings
            
            info_data = {
                'ai_enabled': str(settings.ai_should_be_enabled),
                'enabled_agents': ','.join(getattr(settings, 'ai_enabled_agents', [])),
                'version': '1.0.0'
            }
            
            self.ai_system_info.info(info_data)
            
        except Exception as e:
            logger.error(f"Failed to update AI system info metrics: {e}")
    
    def record_session_created(self, agent_type: str, user_id: str):
        """Record a new AI session creation."""
        if not self.enabled:
            return
        
        try:
            self.ai_sessions_total.labels(agent_type=agent_type, user_id=user_id).inc()
            logger.debug(f"Recorded session creation: {agent_type} for user {user_id}")
        except Exception as e:
            logger.error(f"Failed to record session creation metric: {e}")
    
    def update_active_sessions(self, agent_type: str, count: int):
        """Update the count of active sessions for an agent type."""
        if not self.enabled:
            return
        
        try:
            self.ai_sessions_active.labels(agent_type=agent_type).set(count)
        except Exception as e:
            logger.error(f"Failed to update active sessions metric: {e}")
    
    def record_session_duration(self, agent_type: str, duration_seconds: float):
        """Record the duration of a completed AI session."""
        if not self.enabled:
            return
        
        try:
            self.ai_session_duration.labels(agent_type=agent_type).observe(duration_seconds)
        except Exception as e:
            logger.error(f"Failed to record session duration metric: {e}")
    
    def record_message_processed(self, agent_type: str, message_type: str, role: str):
        """Record a processed AI message."""
        if not self.enabled:
            return
        
        try:
            self.ai_messages_total.labels(
                agent_type=agent_type,
                message_type=message_type,
                role=role
            ).inc()
        except Exception as e:
            logger.error(f"Failed to record message processed metric: {e}")
    
    def record_message_processing_time(self, agent_type: str, duration_seconds: float):
        """Record the time taken to process a message."""
        if not self.enabled:
            return
        
        try:
            self.ai_message_processing_duration.labels(agent_type=agent_type).observe(duration_seconds)
        except Exception as e:
            logger.error(f"Failed to record message processing time metric: {e}")
    
    def record_model_request(self, model_name: str, status: str, response_time_seconds: float = None, tokens_generated: int = None):
        """Record a model inference request."""
        if not self.enabled:
            return
        
        try:
            self.ai_model_requests_total.labels(model_name=model_name, status=status).inc()
            
            if response_time_seconds is not None:
                self.ai_model_response_time.labels(model_name=model_name).observe(response_time_seconds)
            
            if tokens_generated is not None:
                self.ai_model_tokens_generated.labels(model_name=model_name).inc(tokens_generated)
                
        except Exception as e:
            logger.error(f"Failed to record model request metrics: {e}")
    
    def record_cache_hit(self, cache_type: str):
        """Record a memory cache hit."""
        if not self.enabled:
            return
        
        try:
            self.ai_memory_cache_hits.labels(cache_type=cache_type).inc()
        except Exception as e:
            logger.error(f"Failed to record cache hit metric: {e}")
    
    def record_cache_miss(self, cache_type: str):
        """Record a memory cache miss."""
        if not self.enabled:
            return
        
        try:
            self.ai_memory_cache_misses.labels(cache_type=cache_type).inc()
        except Exception as e:
            logger.error(f"Failed to record cache miss metric: {e}")
    
    def update_memory_usage(self, component: str, bytes_used: int):
        """Update memory usage for a component."""
        if not self.enabled:
            return
        
        try:
            self.ai_memory_usage_bytes.labels(component=component).set(bytes_used)
        except Exception as e:
            logger.error(f"Failed to update memory usage metric: {e}")
    
    def record_tool_execution(self, tool_name: str, status: str, duration_seconds: float = None):
        """Record a tool execution."""
        if not self.enabled:
            return
        
        try:
            self.ai_tool_executions_total.labels(tool_name=tool_name, status=status).inc()
            
            if duration_seconds is not None:
                self.ai_tool_execution_duration.labels(tool_name=tool_name).observe(duration_seconds)
                
        except Exception as e:
            logger.error(f"Failed to record tool execution metrics: {e}")
    
    def record_error(self, component: str, error_type: str):
        """Record an AI system error."""
        if not self.enabled:
            return
        
        try:
            self.ai_errors_total.labels(component=component, error_type=error_type).inc()
        except Exception as e:
            logger.error(f"Failed to record error metric: {e}")
    
    def record_health_check(self, duration_seconds: float, component_healths: Dict[str, str]):
        """Record a health check execution."""
        if not self.enabled:
            return
        
        try:
            self.ai_health_check_duration.observe(duration_seconds)
            
            # Update component health statuses
            for component, status in component_healths.items():
                health_value = 1 if status == "healthy" else 0
                self.ai_component_health.labels(component=component).set(health_value)
                
        except Exception as e:
            logger.error(f"Failed to record health check metrics: {e}")
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get a summary of current metrics (for debugging/monitoring)."""
        if not self.enabled:
            return {"error": "Prometheus metrics not available"}
        
        try:
            # This is a simplified summary - in production you'd query the metrics registry
            return {
                "metrics_enabled": True,
                "prometheus_available": True,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"Failed to get metrics summary: {e}")
            return {"error": str(e)}


# Global metrics instance
_ai_metrics: Optional[AIPrometheusMetrics] = None


def get_ai_metrics() -> AIPrometheusMetrics:
    """Get the global AI metrics instance."""
    global _ai_metrics
    if _ai_metrics is None:
        _ai_metrics = AIPrometheusMetrics()
    return _ai_metrics


def record_ai_session_created(agent_type: str, user_id: str):
    """Convenience function to record session creation."""
    metrics = get_ai_metrics()
    metrics.record_session_created(agent_type, user_id)


def record_ai_message_processed(agent_type: str, message_type: str, role: str, processing_time: float = None):
    """Convenience function to record message processing."""
    metrics = get_ai_metrics()
    metrics.record_message_processed(agent_type, message_type, role)
    if processing_time is not None:
        metrics.record_message_processing_time(agent_type, processing_time)


def record_ai_model_request(model_name: str, status: str, response_time: float = None, tokens: int = None):
    """Convenience function to record model requests."""
    metrics = get_ai_metrics()
    metrics.record_model_request(model_name, status, response_time, tokens)


def record_ai_error(component: str, error_type: str):
    """Convenience function to record AI errors."""
    metrics = get_ai_metrics()
    metrics.record_error(component, error_type)


def record_ai_health_check(duration: float, component_healths: Dict[str, str]):
    """Convenience function to record health checks."""
    metrics = get_ai_metrics()
    metrics.record_health_check(duration, component_healths)